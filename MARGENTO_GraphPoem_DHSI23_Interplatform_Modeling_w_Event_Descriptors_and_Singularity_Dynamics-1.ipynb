{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c7acf6-753b-43b2-9bf2-8e8df32a4eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Synthetic log simulator for #GraphPoem (A4, B1) - streamlined and defensive\n",
    "# This script regenerates synthetic logs and outputs of #graphpoem @ dhsi23.\n",
    "\n",
    "import json, random, math, os\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Anchors as extracted from \n",
    "# https://www.taylorfrancis.com/chapters/edit/10.4324/9781003320838-3/dynamical-systems-interplatform-intermediality-chris-tanasescu?context=ubx&refId=a352b9ec-5706-43ce-84c4-827290868465 \n",
    "anchors = [\n",
    "    ((\"7:09\",\"7:27\"), (\"8:04\",\"8:55\")),\n",
    "    ((\"17:36\",\"18:43\"), (None, None)),\n",
    "    ((\"1:08:04\",\"1:08:18\"), (\"1:08:24\",\"1:09:00\")),\n",
    "    ((\"1:09:45\",\"1:10:36\"), (\"1:11:07\",\"1:12:16\")),\n",
    "]\n",
    "\n",
    "def mmss_to_seconds(s):\n",
    "    if s is None: return None\n",
    "    parts = s.split(':')\n",
    "    if len(parts) == 2:\n",
    "        return int(parts[0]) * 60 + int(parts[1])\n",
    "    elif len(parts) == 3:\n",
    "        return int(parts[0]) * 3600 + int(parts[1]) * 60 + int(parts[2])\n",
    "    else:\n",
    "        return int(parts[0])\n",
    "\n",
    "event_windows = []\n",
    "trail_windows = []\n",
    "for ev, tr in anchors:\n",
    "    ev_start = mmss_to_seconds(ev[0]); ev_end = mmss_to_seconds(ev[1])\n",
    "    event_windows.append((ev_start, ev_end))\n",
    "    if tr[0] is None:\n",
    "        trail_windows.append(None)\n",
    "    else:\n",
    "        trail_windows.append((mmss_to_seconds(tr[0]), mmss_to_seconds(tr[1])))\n",
    "\n",
    "# Horizon\n",
    "max_time = 0\n",
    "\n",
    "for s,e in event_windows:\n",
    "    if e and e>max_time: max_time = e\n",
    "for tr in trail_windows:\n",
    "    if tr and tr[1] and tr[1]>max_time: max_time = tr[1]\n",
    "        \n",
    "HORIZON = max_time + 600\n",
    "DT = 10\n",
    "time_points = np.arange(0, HORIZON+DT, DT)\n",
    "\n",
    "# Users\n",
    "twitter_users = [f\"tw_user_{i+1}\" for i in range(11)]\n",
    "fb_overlap = twitter_users[:10]\n",
    "fb_only_count = 37 - len(fb_overlap)\n",
    "fb_users = fb_overlap + [f\"fb_user_{i+1}\" for i in range(fb_only_count)]\n",
    "jh_contributors = [f\"jh_user_{i+1}\" for i in range(101)]\n",
    "twitter_dormant = [f\"tw_dorm_{i+1}\" for i in range(200)]\n",
    "\n",
    "# Poems\n",
    "# num_poems = 2312\n",
    "import os\n",
    "\n",
    "POEM_FOLDER = \"poems_#dhsi25\"\n",
    "\n",
    "poems = sorted([f for f in os.listdir(POEM_FOLDER) if f.endswith(\".txt\")])\n",
    "NUM_POEMS = len(poems)\n",
    "\n",
    "placeholder_map = {\n",
    "    0: {\n",
    "        \"before\": \"afrikaans_emoji_in_the_scriptorium_slippers_trans_inggs.txt\",\n",
    "        \"during\": \"eva_tizzani_kakeche_from_spanish_trans_david_brunson_poem_1.txt\",\n",
    "        \"after\": \"tan_chee_lay_iv_altering_constituencies_from_chinese_trans_teng_qian_xi_poem_4.txt\"\n",
    "    },\n",
    "    1: {\n",
    "        \"before\": \"vítězslav_nezval_a_man_composing_a_selfportrait_out_of_objects_from_czech_trans_stephan_delbos_and_tereza_novická_poem_1.txt\",\n",
    "        \"during\": \"martinus_nijhoff_trans_james_s_holmes_awater_excerpt.txt\",\n",
    "        \"after\": \"w_h_auden_brussels_in_winter.txt\"\n",
    "    },\n",
    "    2: {\n",
    "        \"before\": \"anonymous_libra_from_old_trans_gnaomi_siemens_poem_7.txt\",\n",
    "        \"during\": \"margento_after-ovid's-elegy.txt\",\n",
    "        \"after\": \"place_serban-foarta_trans-margento_place-de-la-concorde_paris-&-france-&-bucharest-&-romania.txt\"\n",
    "    },\n",
    "    3: {\n",
    "        \"before\": \"place_flavia-teoc_trans-margento_constantinople_byzantium-&-turkey.txt\",\n",
    "        \"during\": \"hayashi_amari_from_scent_of_nanako_from_japanese_trans_jon_holt_poem_1.txt\",\n",
    "        \"after\": \"constantine_p_cavafy_returning_home_from_greece_from_greek_trans_george_economou_poem_2.txt\"\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "anchor_positions = np.linspace(0, NUM_POEMS-1, 4)\n",
    "event_indices = []\n",
    "\n",
    "for anchor in anchor_positions:\n",
    "    center_idx = int(anchor)\n",
    "    before_idx = max(0, center_idx - 2)\n",
    "    during_idx = center_idx\n",
    "    after_idx = min(NUM_POEMS-1, center_idx + 2)\n",
    "    event_indices.append((before_idx, during_idx, after_idx))\n",
    "\n",
    "# Override poems with your real event poems\n",
    "for idx_tuple, mapping in zip(event_indices, placeholder_map.values()):\n",
    "    b, d, a = idx_tuple\n",
    "    poems[b] = mapping[\"before\"]\n",
    "    poems[d] = mapping[\"during\"]\n",
    "    poems[a] = mapping[\"after\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71326e2f-4397-4550-aa23-d563c958343d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "poem_seq_path = \"margento_#graphpoem_dhsi23_poem_sequence.json\"\n",
    "\n",
    "with open(poem_seq_path, \"w\") as f:\n",
    "    json.dump({\n",
    "        \"poems\": poems,\n",
    "        \"event_indices\": event_indices,\n",
    "        \"event_poems\": placeholder_map\n",
    "    }, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea085e31-86bf-466c-a38c-1e297a76ecd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Bot schedule\n",
    "bot_interval = 180\n",
    "bot_jitter = 5\n",
    "bot_times = []\n",
    "t = 0\n",
    "\n",
    "while t <= HORIZON:\n",
    "    jitter = random.randint(-bot_jitter, bot_jitter)\n",
    "    tt = max(0, t + jitter)\n",
    "    bot_times.append(tt)\n",
    "    t += bot_interval\n",
    "    \n",
    "bot_times = sorted(list(set(bot_times)))\n",
    "\n",
    "# WE WILL CONTINUE DEVELOPING THE BOT ONCE WE ARE DONE W| JUPYTERHUB SINCE THE BOT TWEETS THE POEMS PROVIDED BY THE CODE THERE\n",
    "\n",
    "# helpers\n",
    "def in_any_event(t):\n",
    "    for s,e in event_windows:\n",
    "        if s is not None and e is not None and s <= t <= e:\n",
    "            return True\n",
    "    return False\n",
    "    \n",
    "def in_any_trail(t):\n",
    "    for tr in trail_windows:\n",
    "        if tr and tr[0] is not None and tr[1] is not None and tr[0] <= t <= tr[1]:\n",
    "            return True\n",
    "    return False\n",
    "    \n",
    "def which_event(t):\n",
    "    for idx,(s,e) in enumerate(event_windows):\n",
    "        if s is not None and e is not None and s <= t <= e:\n",
    "            return idx+1\n",
    "    return None\n",
    "\n",
    "\n",
    "# Jupyter logs: map poems to times denser near events (A4)\n",
    "poem_times = {}\n",
    "base_spacing = HORIZON / len(poems)\n",
    "\n",
    "for i, pname in enumerate(poems):\n",
    "    base_t = int(i * base_spacing)\n",
    "    # find nearest event center\n",
    "    mindist = 1e9; nearest_ev = None\n",
    "    for s,e in event_windows:\n",
    "        center = (s+e)/2.0\n",
    "        d = abs(base_t - center)\n",
    "        if d < mindist:\n",
    "            mindist = d; nearest_ev = (s,e,center)\n",
    "    if mindist < 0.15 * HORIZON:\n",
    "        center = int(nearest_ev[2])\n",
    "        tloc = int(center + random.randint(-30,30) + random.randint(-int(0.05*HORIZON), int(0.05*HORIZON))//10)\n",
    "    else:\n",
    "        tloc = base_t + random.randint(-30,30)\n",
    "    poem_times[pname] = max(0, min(HORIZON, int(tloc)))\n",
    "\n",
    "jupyter_rows = []\n",
    "file_id = 0\n",
    "\n",
    "for pname in poems:\n",
    "    tloc = poem_times[pname]\n",
    "\n",
    "    # Set number of contributors based on event/trail/normal regions\n",
    "    if in_any_event(tloc):\n",
    "        n_contrib = random.randint(3, 8)\n",
    "    elif in_any_trail(tloc):\n",
    "        n_contrib = random.randint(2, 5)\n",
    "    else:\n",
    "        n_contrib = random.randint(1, 3)\n",
    "\n",
    "    # Choose distinct contributors\n",
    "    chosen_contributors = random.sample(\n",
    "        jh_contributors, \n",
    "        k=min(len(jh_contributors), n_contrib)\n",
    "    )\n",
    "\n",
    "    # Create one \"save\" event per contributor\n",
    "    for user in chosen_contributors:\n",
    "        file_id += 1\n",
    "        jupyter_rows.append({\n",
    "            \"time\": int(tloc) + random.randint(0, 40),\n",
    "            \"user\": user,\n",
    "            \"action\": \"save\",\n",
    "            \"file\": pname,\n",
    "            \"event\": which_event(tloc)\n",
    "        })\n",
    "\n",
    "df_jh = pd.DataFrame(jupyter_rows)\n",
    "jh_path = \"margento_#graphpoem_dhsi23_synthetic_jupyter_log.csv\"\n",
    "df_jh.sort_values(by=\"time\").to_csv(jh_path, index=False)\n",
    "\n",
    "# Twitter generation\n",
    "twitter_rows = []\n",
    "tweet_records = []\n",
    "tweet_id = 0\n",
    "\n",
    "for tt in bot_times:\n",
    "    tweet_id += 1\n",
    "    # Ensure we have saved poems before picking\n",
    "    saved_poems_sorted_by_time = sorted(jupyter_rows, key=lambda r: r[\"time\"])\n",
    "    # When the bot tweets at time tt, choose the *latest* poem saved before tt\n",
    "    available_poems = [r[\"file\"] for r in saved_poems_sorted_by_time if r[\"time\"] <= tt]\n",
    "    if available_poems:\n",
    "        poem_name = available_poems[-1]  # most recent poem\n",
    "    else:\n",
    "        poem_name = random.choice(poems) # fallback\n",
    "    media_flag = True\n",
    "    tweet_meta = {\"tweet_id\": f\"t{tweet_id:05d}\", \"time\": int(tt), \"poem\": poem_name, \"media\": media_flag}\n",
    "    tweet_records.append(tweet_meta)\n",
    "    base_likers = random.sample(twitter_users, k=random.randint(1,3))\n",
    "    base_rts = random.sample(twitter_users, k=random.choice([0,1]))\n",
    "    base_dorm_likes = random.sample(twitter_dormant, k=random.randint(0,2))\n",
    "    if in_any_event(tt) or in_any_trail(tt):\n",
    "        if in_any_event(tt):\n",
    "            extra = random.randint(4,12)\n",
    "            extra_rts = random.randint(2,6)\n",
    "        else:\n",
    "            extra = random.randint(2,6)\n",
    "            extra_rts = random.randint(1,3)\n",
    "        extra_likers = random.sample(twitter_dormant, k=min(len(twitter_dormant), extra))\n",
    "    else:\n",
    "        extra_likers = []\n",
    "        extra_rts = 0\n",
    "    twitter_rows.append({\"time\": int(tt), \"user\":\"bot_twitter\", \"action\":\"tweet\", \"object_id\": tweet_meta[\"tweet_id\"], \"poem\": poem_name, \"media\": media_flag, \"event\": which_event(tt)})\n",
    "    for u in base_likers:\n",
    "        twitter_rows.append({\"time\": int(tt)+random.randint(0,8), \"user\":u, \"action\":\"like\", \"object_id\": tweet_meta[\"tweet_id\"], \"poem\": poem_name, \"media\": media_flag, \"event\": which_event(tt)})\n",
    "    for u in base_dorm_likes:\n",
    "        twitter_rows.append({\"time\": int(tt)+random.randint(1,30), \"user\":u, \"action\":\"like\", \"object_id\": tweet_meta[\"tweet_id\"], \"poem\": poem_name, \"media\": media_flag, \"event\": which_event(tt)})\n",
    "    for u in base_rts:\n",
    "        twitter_rows.append({\"time\": int(tt)+random.randint(0,20), \"user\":u, \"action\":\"retweet\", \"object_id\": tweet_meta[\"tweet_id\"], \"poem\": poem_name, \"media\": media_flag, \"event\": which_event(tt)})\n",
    "    for u in extra_likers:\n",
    "        twitter_rows.append({\"time\": int(tt)+random.randint(0,40), \"user\":u, \"action\":\"like\", \"object_id\": tweet_meta[\"tweet_id\"], \"poem\": poem_name, \"media\": media_flag, \"event\": which_event(tt)})\n",
    "    for _ in range(extra_rts):\n",
    "        u = random.choice(twitter_users + twitter_dormant)\n",
    "        twitter_rows.append({\"time\": int(tt)+random.randint(0,60), \"user\":u, \"action\":\"retweet\", \"object_id\": tweet_meta[\"tweet_id\"], \"poem\": poem_name, \"media\": media_flag, \"event\": which_event(tt)})\n",
    "\n",
    "df_tw = pd.DataFrame(twitter_rows)\n",
    "tw_path = \"margento_#graphpoem_dhsi23_synthetic_twitter_log.csv\"\n",
    "df_tw.sort_values(by=\"time\").to_csv(tw_path, index=False)\n",
    "\n",
    "\n",
    "# Facebook live simulation\n",
    "facebook_rows = []\n",
    "\n",
    "for tt in range(0, HORIZON+1, DT):\n",
    "    if in_any_event(tt) or in_any_trail(tt) or (tt in bot_times):\n",
    "        if in_any_event(tt):\n",
    "            num_actions = random.randint(3,12)\n",
    "        elif in_any_trail(tt):\n",
    "            num_actions = random.randint(1,6)\n",
    "        else:\n",
    "            num_actions = random.randint(0,2)\n",
    "        for _ in range(num_actions):\n",
    "            actor = random.choice(fb_users + twitter_users[:5])\n",
    "            action = random.choices([\"like_live\",\"share_live\",\"comment_live\"], weights=[0.8,0.15,0.05])[0]\n",
    "            facebook_rows.append({\"time\": int(tt)+random.randint(0,30), \"user\": actor, \"action\": action, \"object\":\"livestream\", \"event\": which_event(tt)})\n",
    "\n",
    "df_fb = pd.DataFrame(facebook_rows)\n",
    "fb_path = \"margento_#graphpoem_dhsi23_synthetic_facebook_log.csv\"\n",
    "df_fb.sort_values(by=\"time\").to_csv(fb_path, index=False)\n",
    "\n",
    "with open(\"margento_#graphpoem_dhsi23_facebook_post_event_summary.json\", \"w\") as f:\n",
    "    json.dump({\"viewers_2weeks\":11500,\"likes\":244,\"shares\":3,\"comments\":11}, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c13ed356-49c7-4328-a4f2-62409f451a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Event descriptors\n",
    "import random\n",
    "\n",
    "event_descriptors = []\n",
    "\n",
    "for ei, ((s, e), tr) in enumerate(zip(event_windows, trail_windows), start=1):\n",
    "    # Twitter slice\n",
    "    tw_slice = df_tw[(df_tw[\"time\"] >= s) & (df_tw[\"time\"] <= e)]\n",
    "    num_tw_likes = int((tw_slice[\"action\"] == \"like\").sum())\n",
    "    num_tw_rts = int((tw_slice[\"action\"] == \"retweet\").sum())\n",
    "    \n",
    "    # Facebook slice\n",
    "    fb_slice = df_fb[(df_fb[\"time\"] >= s) & (df_fb[\"time\"] <= e)]\n",
    "    num_fb_likes = int((fb_slice[\"action\"] == \"like_live\").sum())\n",
    "    num_fb_shares = int((fb_slice[\"action\"] == \"share_live\").sum())\n",
    "    \n",
    "    # JupyterHub slice\n",
    "    jh_slice = df_jh[(df_jh[\"time\"] >= s) & (df_jh[\"time\"] <= e)]\n",
    "    num_jh_saves = len(jh_slice)\n",
    "    contributors = sorted(list(set(jh_slice[\"user\"].tolist())))\n",
    "    \n",
    "    # All poems in the window\n",
    "    poems_in_window = jh_slice[\"file\"].tolist()\n",
    "    \n",
    "    # Only the latest file per unique time\n",
    "    poems_in_window_selected = (\n",
    "        jh_slice.sort_values(\"time\")  # ensure sorted by time\n",
    "                .groupby(\"time\", as_index=False)  # group by time\n",
    "                .last()  # take last file for each time\n",
    "                [\"file\"]\n",
    "                .tolist()\n",
    "    )\n",
    "    \n",
    "    # Build descriptor\n",
    "    descriptor = {\n",
    "        \"event_index\": ei,\n",
    "        \"event_window\": (s, e),\n",
    "        \"trail_window\": tr,\n",
    "        \"num_tw_likes\": num_tw_likes,\n",
    "        \"num_tw_rts\": num_tw_rts,\n",
    "        \"num_fb_likes\": num_fb_likes,\n",
    "        \"num_fb_shares\": num_fb_shares,\n",
    "        \"num_jh_saves\": num_jh_saves,\n",
    "        \"contributors\": contributors,\n",
    "        \"poems_in_window\": poems_in_window,\n",
    "        \"poems_in_window_selected\": poems_in_window_selected,\n",
    "        \"highlighted_poems_before_during_after\": placeholder_map[ei-1],\n",
    "        \"semantic_tags\": [\"intermedia\",\"dissonance\"] if random.random() < 0.5 else [\"collage\",\"sampling\"],\n",
    "        \"media_density\": float(min(1.0, (num_tw_likes + num_tw_rts)/10.0))\n",
    "    }\n",
    "    \n",
    "    event_descriptors.append(descriptor)\n",
    "\n",
    "ed_path = \"margento_#graphpoem_dhsi23_event_descriptors.json\"\n",
    "with open(ed_path, \"w\") as f:\n",
    "    json.dump(event_descriptors, f, indent=2)\n",
    "\n",
    "# Simple descriptor encoder and beta_R approximator (resource bounded deterministic approx)\n",
    "def simple_descriptor_encoder(descriptor, salt=0):\n",
    "    s = json.dumps(descriptor, sort_keys=True)\n",
    "    h = abs(hash(s + str(salt)))\n",
    "    return h % (10**6)\n",
    "\n",
    "def halting_bits_resource_bounded_simple(indices, R):\n",
    "    M_R = max(2, 100 + R*10)\n",
    "    K_R = max(1, min(M_R-1, int(0.1 * M_R)))\n",
    "    bits = {}\n",
    "    for idx in indices:\n",
    "        bits[idx] = 1 if (idx % M_R) < K_R else 0\n",
    "    return bits\n",
    "\n",
    "descriptor_indices = [simple_descriptor_encoder(d, salt=0) for d in event_descriptors]\n",
    "for di, d in zip(descriptor_indices, event_descriptors):\n",
    "    d['descriptor_index'] = int(di)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ec71325d-2b47-4367-a6ad-a9ba139bafea",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ed_path, \"w\") as f:\n",
    "    json.dump(event_descriptors, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e390634-cd72-4811-876a-6c4adaf83f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_beta_R(time_vec, event_descriptors, R, slot_width=30.0):\n",
    "    indices = [d['descriptor_index'] for d in event_descriptors]\n",
    "    bits_map = halting_bits_resource_bounded_simple(indices, R)\n",
    "    beta = np.zeros_like(time_vec, dtype=float)\n",
    "    for d in event_descriptors:\n",
    "        k = d['descriptor_index']\n",
    "        bit = bits_map.get(k, 0)\n",
    "        if bit == 1:\n",
    "            s,e = d['event_window']\n",
    "            center = (s+e)/2.0\n",
    "            kernel = np.exp(-0.5 * ((time_vec - center)/slot_width)**2)\n",
    "            beta += kernel\n",
    "    if beta.max() > 0:\n",
    "        beta = beta / beta.max()\n",
    "    return beta\n",
    "\n",
    "time_vec = np.array(list(time_points))\n",
    "beta_1 = make_beta_R(time_vec, event_descriptors, R=1)\n",
    "beta_10 = make_beta_R(time_vec, event_descriptors, R=10)\n",
    "np.save(\"margento_#graphpoem_dhsi23_beta_R_1.npy\", beta_1)\n",
    "np.save(\"margento_#graphpoem_dhsi23_beta_R_10.npy\", beta_10)\n",
    "\n",
    "# Save files manifest\n",
    "manifest = {\n",
    "    \"twitter_log\": tw_path,\n",
    "    \"jupyter_log\": jh_path,\n",
    "    \"facebook_log\": fb_path,\n",
    "    \"poem_sequence\": poem_seq_path,\n",
    "    \"event_descriptors\": ed_path,\n",
    "    \"beta_R1\": \"margento_#graphpoem_dhsi23_beta_R_1.npy\",\n",
    "    \"beta_R10\": \"margento_#graphpoem_dhsi23_beta_R_10.npy\"\n",
    "}\n",
    "with open(\"margento_#graphpoem_dhsi23_synthetic_manifest.json\", \"w\") as f:\n",
    "    json.dump(manifest, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0db62e-8cec-4b53-b8fe-ecbdba59dbf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf55a402-a3d3-426b-8977-1fe3e535b4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot a small timeline\n",
    "plt.figure(figsize=(10,4))\n",
    "tweet_times = np.array([r['time'] for r in tweet_records])\n",
    "plt.plot(tweet_times/60.0, np.ones_like(tweet_times)*0.2, '|', label='bot tweets', color='tab:blue')\n",
    "tw_like_times = df_tw[df_tw['action']=='like']['time'].values\n",
    "tw_counts, _ = np.histogram(tw_like_times, bins=time_points)\n",
    "# plt.plot(time_points[:-1]/60.0, tw_counts/np.max(tw_counts+1)*0.9, label='twitter likes (norm)', color='tab:orange')\n",
    "plt.plot(time_points[:-1]/60.0, tw_counts/np.max(tw_counts+1)*0.9, label='twitter likes (norm)', color='black')\n",
    "jh_times = df_jh['time'].values\n",
    "jh_counts, _ = np.histogram(jh_times, bins=time_points)\n",
    "plt.plot(time_points[:-1]/60.0, jh_counts/np.max(jh_counts+1)*0.7, label='jupyter saves (norm)', color='tab:green')\n",
    "if not df_fb.empty:\n",
    "    fb_times = df_fb['time'].values\n",
    "    fb_counts, _ = np.histogram(fb_times, bins=time_points)\n",
    "    plt.plot(time_points[:-1]/60.0, fb_counts/np.max(fb_counts+1)*0.6, label='fb actions (norm)', color='tab:red')\n",
    "#plt.plot(time_vec/60.0, beta_1*0.95, label='beta_R=1', color='k', linestyle='--')\n",
    "plt.plot(time_vec/60.0, beta_1*0.95, label='beta_R=1', color='yellow', linestyle='--')\n",
    "# plt.plot(time_vec/60.0, beta_10*0.6, label='beta_R=10', color='k', linestyle=':')\n",
    "plt.plot(time_vec/60.0, beta_10*0.6, label='beta_R=10', color='yellow', linestyle=':')\n",
    "for (s,e) in event_windows:\n",
    "    plt.axvspan(s/60.0, e/60.0, color='gray', alpha=0.12)\n",
    "for tr in trail_windows:\n",
    "    if tr:\n",
    "        plt.axvspan(tr[0]/60.0, tr[1]/60.0, color='gray', alpha=0.06)\n",
    "plt.xlabel('time (minutes)'); plt.ylabel('activity / beta'); plt.legend(loc='upper right', bbox_to_anchor=(1.3,1.05))\n",
    "plt.tight_layout()\n",
    "plot_path = \"margento_#graphpoem_dhsi23_synthetic_timeline_1.png\"\n",
    "plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "823ac2a2-572d-4774-857a-e1a1108dc085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated synthetic logs and artifacts. Manifest: margento_#graphpoem_dhsi23_synthetic_manifest.json\n",
      "{\n",
      "  \"twitter_log\": \"margento_#graphpoem_dhsi23_synthetic_twitter_log.csv\",\n",
      "  \"jupyter_log\": \"margento_#graphpoem_dhsi23_synthetic_jupyter_log.csv\",\n",
      "  \"facebook_log\": \"margento_#graphpoem_dhsi23_synthetic_facebook_log.csv\",\n",
      "  \"poem_sequence\": \"margento_#graphpoem_dhsi23_poem_sequence.json\",\n",
      "  \"event_descriptors\": \"margento_#graphpoem_dhsi23_event_descriptors.json\",\n",
      "  \"beta_R1\": \"margento_#graphpoem_dhsi23_beta_R_1.npy\",\n",
      "  \"beta_R10\": \"margento_#graphpoem_dhsi23_beta_R_10.npy\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'twitter_log': 'margento_#graphpoem_dhsi23_synthetic_twitter_log.csv',\n",
       " 'jupyter_log': 'margento_#graphpoem_dhsi23_synthetic_jupyter_log.csv',\n",
       " 'facebook_log': 'margento_#graphpoem_dhsi23_synthetic_facebook_log.csv',\n",
       " 'poem_sequence': 'margento_#graphpoem_dhsi23_poem_sequence.json',\n",
       " 'event_descriptors': 'margento_#graphpoem_dhsi23_event_descriptors.json',\n",
       " 'beta_R1': 'margento_#graphpoem_dhsi23_beta_R_1.npy',\n",
       " 'beta_R10': 'margento_#graphpoem_dhsi23_beta_R_10.npy'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Generated synthetic logs and artifacts. Manifest:\", \"margento_#graphpoem_dhsi23_synthetic_manifest.json\")\n",
    "print(json.dumps(manifest, indent=2))\n",
    "manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16587773-aefe-43c7-a583-50b20ec4d99d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63019f12-f7dd-4710-b348-7944b479e01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# BUILDING ADJACENCY MATRICES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44640202-b192-4183-8fc5-031fc7c33c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 1. BUILD NODE LIST\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "node_index = {n: i for i, n in enumerate(nodes)}\n",
    "\n",
    "all_users = sorted(\n",
    "    set(df_tw[\"user\"]).union(df_fb[\"user\"]).union(df_jh[\"user\"])\n",
    ")\n",
    "\n",
    "all_poems = sorted(set(poems))\n",
    "\n",
    "nodes = all_users + all_poems\n",
    "\n",
    "# Add livestream node\n",
    "if \"livestream\" not in nodes:\n",
    "    nodes.append(\"livestream\")\n",
    "    \n",
    "node_index = {n: i for i, n in enumerate(nodes)}\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 2. SOCIAL INTERACTION ADJACENCY (Twitter + Facebook)\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "A_social = np.zeros((len(nodes), len(nodes)), dtype=float)\n",
    "\n",
    "# Twitter likes/retweets: user -> tweet/poem\n",
    "for _, row in df_tw.iterrows():\n",
    "    u = row[\"user\"]\n",
    "    if u == \"bot_twitter\":\n",
    "        continue\n",
    "    f = row[\"poem\"]\n",
    "    if u in node_index and f in node_index:\n",
    "        A_social[node_index[u], node_index[f]] += 1\n",
    "\n",
    "# Facebook likes/shares\n",
    "for _, row in df_fb.iterrows():\n",
    "    u = row[\"user\"]\n",
    "    if row[\"action\"] in {\"like_live\", \"share_live\"}:\n",
    "        if u in node_index:\n",
    "            A_social[node_index[u], node_index[\"livestream\"]] = 1\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 3. TEMPORAL COACTIVITY ADJACENCY\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "A_temp = np.zeros((len(nodes), len(nodes)), dtype=float)\n",
    "WINDOW = 30  # seconds\n",
    "\n",
    "def add_temporal_edges(df):\n",
    "    rows = df.sort_values(\"time\")\n",
    "    for _, r in rows.iterrows():\n",
    "        u = r[\"user\"]\n",
    "        t = r[\"time\"]\n",
    "        # find all actions within +/- WINDOW\n",
    "        mask = (rows[\"time\"] >= t - WINDOW) & (rows[\"time\"] <= t + WINDOW)\n",
    "        block = rows[mask]\n",
    "        for _, r2 in block.iterrows():\n",
    "            u2 = r2[\"user\"]\n",
    "            if u != u2 and u in node_index and u2 in node_index:\n",
    "                A_temp[node_index[u], node_index[u2]] += 1\n",
    "\n",
    "# Add from all platforms\n",
    "add_temporal_edges(df_tw)\n",
    "add_temporal_edges(df_fb)\n",
    "add_temporal_edges(df_jh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dce956fc-da43-4053-96f7-6f08f30fdadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.save(\"margento_graphpoem_dhsi23_adjacency_matrix_before_semantics_and_form.npy\", A_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5c3af7-2288-4586-af4d-d74d5833c7ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcec4fe-111c-4e78-9c66-a517008210eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------------------------------------\n",
    "# 4. SEMANTIC and SONIC SIMILARITY ADJACENCY\n",
    "# -----------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51af7410-09c8-494f-bbfd-72eb69d2b838",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2badf50a-cb50-430f-88d3-5ac3477a19d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/Margento/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Margento/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package omw to /Users/Margento/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/Margento/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# HELPERS\n",
    "\n",
    "import re, unicodedata, math\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Download Unicode Scripts.txt\n",
    "SCRIPTS_URL = \"https://www.unicode.org/Public/UCD/latest/ucd/Scripts.txt\"\n",
    "\n",
    "\n",
    "# ALL SCRIPTS 'UNDER THE SUN' [IN UNICODE, THAT IS]\n",
    "\n",
    "import regex\n",
    "import urllib.request\n",
    "\n",
    "\n",
    "def get_all_scripts() -> set[str]:\n",
    "    \"\"\"\n",
    "    Fetch the official Unicode script names from Scripts.txt.\n",
    "    \"\"\"\n",
    "    with urllib.request.urlopen(SCRIPTS_URL) as f:\n",
    "        lines = f.read().decode(\"utf-8\").splitlines()\n",
    "\n",
    "    scripts = set()\n",
    "    for line in lines:\n",
    "        if line.strip() and not line.startswith(\"#\"):\n",
    "            # Example line: \"0041..005A; Latin # L&  [26] LATIN CAPITAL LETTER A..Z\"\n",
    "            parts = line.split(\";\")\n",
    "            if len(parts) >= 2:\n",
    "                script = parts[1].strip().split()[0]\n",
    "                scripts.add(script)\n",
    "    return scripts\n",
    "\n",
    "UNICODE_SCRIPTS = sorted(get_all_scripts())\n",
    "\n",
    "def char_script(ch):\n",
    "    import regex\n",
    "    \n",
    "    if not ch or len(ch) != 1:\n",
    "        return \"INVALID\"\n",
    "\n",
    "    for script in UNICODE_SCRIPTS:\n",
    "        try:\n",
    "            # Use the script name exactly as Unicode defines it\n",
    "            if regex.match(rf\"\\p{{Script={script}}}\", ch):\n",
    "                return script  # return it as-is\n",
    "        except regex.error:\n",
    "            continue  # skip invalid/unrecognized scripts\n",
    "\n",
    "    return \"UNKNOWN\"\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def word_script(word: str) -> str:\n",
    "    \"\"\"\n",
    "    Return the dominant script of a word (based on majority of alphabetic chars).\n",
    "    \"\"\"\n",
    "    scripts = Counter(char_script(ch) for ch in word if ch.isalpha())\n",
    "    return scripts.most_common(1)[0][0] if scripts else \"OTHER\"\n",
    "\n",
    "\n",
    "def get_unicode_name(ch):\n",
    "    try:\n",
    "        return unicodedata.name(ch)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "import unicodedata\n",
    "\n",
    "# Latin/Cyrillic/Greek/Devanagari vowels (extendable)\n",
    "_vowel_re_latin = re.compile(r\"[aeiouy\\u00E0-\\u00FF]+\", re.IGNORECASE)\n",
    "_vowel_re_cyrillic = re.compile(r\"[аеёиоуыэюя]+\", re.IGNORECASE)  # basic Russian vowels\n",
    "_vowel_re_greek = re.compile(r\"[αεηιουωάέήίόύώ]\", re.IGNORECASE)   # modern Greek vowels\n",
    "_vowel_re_devanagari = re.compile(r\"[अआइईउऊएऐओऔऋॠॡॢॣ]\", re.IGNORECASE)\n",
    "\n",
    "\n",
    "def approx_syllables_word(word: str) -> int:\n",
    "    if not word:\n",
    "        return 0\n",
    "    w = unicodedata.normalize(\"NFC\", word)\n",
    "    script = word_script(w)\n",
    "\n",
    "    if script == \"LATIN\":\n",
    "        groups = _vowel_re_latin.findall(w)\n",
    "        count = len(groups)\n",
    "        if w.lower().endswith(\"e\") and count > 1:  # silent 'e'\n",
    "            count -= 1\n",
    "        return max(1, count)\n",
    "\n",
    "    if script == \"CYRILLIC\":\n",
    "        groups = _vowel_re_cyrillic.findall(w)\n",
    "        return max(1, len(groups))\n",
    "\n",
    "    if script == \"GREEK\":\n",
    "        groups = _vowel_re_greek.findall(w)\n",
    "        return max(1, len(groups))\n",
    "\n",
    "    if script == \"DEVANAGARI\":\n",
    "        groups = _vowel_re_devanagari.findall(w)\n",
    "        return max(1, len(groups))\n",
    "\n",
    "    if script in (\"HIRAGANA\", \"KATAKANA\"):\n",
    "        kana_chars = [ch for ch in w if '\\u3040' <= ch <= '\\u30FF']\n",
    "        return max(1, len(kana_chars))\n",
    "\n",
    "    if script == \"HANGUL\":\n",
    "        return len([ch for ch in w if '\\uAC00' <= ch <= '\\uD7A3'])\n",
    "\n",
    "    if script == \"CJK\":\n",
    "        chars = [ch for ch in w if '\\u4E00' <= ch <= '\\u9FFF']\n",
    "        return max(1, len(chars))\n",
    "\n",
    "    if script == \"THAI\":\n",
    "        return max(1, len([ch for ch in w if ch.strip()]))\n",
    "\n",
    "    # Fallback\n",
    "    groups = _vowel_re_latin.findall(w)\n",
    "    return max(1, len(groups) if groups else len(w))\n",
    "\n",
    "\n",
    "def extract_phonological_clusters(word: str):\n",
    "    clusters = set()\n",
    "    w = unicodedata.normalize(\"NFC\", word.lower())\n",
    "    script = word_script(w)\n",
    "\n",
    "    if script in (\"LATIN\", \"GREEK\", \"CYRILLIC\"):\n",
    "        consonant_matches = re.findall(r'[^aeiouy]+', w)\n",
    "        for c in consonant_matches:\n",
    "            for i in range(len(c)):\n",
    "                for j in range(i+1, len(c)+1):\n",
    "                    clusters.add(c[i:j])\n",
    "        vowel_matches = re.findall(r'[aeiouy]+', w)\n",
    "        for v in vowel_matches:\n",
    "            for i in range(len(v)):\n",
    "                for j in range(i+1, len(v)+1):\n",
    "                    clusters.add(v[i:j])\n",
    "        for k in range(2, 5):\n",
    "            if len(w) >= k:\n",
    "                clusters.add(w[-k:])\n",
    "\n",
    "    elif script in (\"ARABIC\", \"HEBREW\"):\n",
    "        consonant_runs = re.findall(r'[^aeiou]+', w)\n",
    "        for c in consonant_runs:\n",
    "            for i in range(len(c)):\n",
    "                for j in range(i+1, len(c)+1):\n",
    "                    clusters.add(c[i:j])\n",
    "        for k in range(2, 5):\n",
    "            if len(w) >= k:\n",
    "                clusters.add(w[-k:])\n",
    "\n",
    "    elif script == \"DEVANAGARI\":\n",
    "        groups = _vowel_re_devanagari.findall(w)\n",
    "        for g in groups:\n",
    "            clusters.add(g)\n",
    "        for k in range(2, 5):\n",
    "            if len(w) >= k:\n",
    "                clusters.add(w[-k:])\n",
    "\n",
    "    elif script in (\"HIRAGANA\", \"KATAKANA\", \"HANGUL\", \"CJK\"):\n",
    "        chars = list(w)\n",
    "        clusters.update(chars)\n",
    "        for i in range(len(chars)-1):\n",
    "            clusters.add(chars[i] + chars[i+1])\n",
    "\n",
    "    else:\n",
    "        for i in range(len(w)):\n",
    "            for j in range(i+1, min(i+4, len(w))+1):\n",
    "                clusters.add(w[i:j])\n",
    "\n",
    "    return clusters\n",
    "\n",
    "\n",
    "_word_re = re.compile(r\"\\w+\", re.UNICODE)\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for m in _word_re.finditer(text):\n",
    "        tok = m.group(0)\n",
    "        tokens.append(tok)\n",
    "    return tokens\n",
    "\n",
    "_fricatives = set(list(\"fvsz\") + [\"sh\",\"zh\",\"th\"])\n",
    "_plosives = set(list(\"pbtdkg\"))\n",
    "\n",
    "def phonetic_density(tokens):\n",
    "    latin_tokens = [t for t in tokens if char_script(t[0]) == \"LATIN\"]\n",
    "    joined = \" \".join(latin_tokens).lower()\n",
    "    letters = re.sub(r'[^a-z]', '', joined)\n",
    "    if not letters:\n",
    "        return 0.0, 0.0, 0.0\n",
    "    fric_count = sum(joined.count(f) for f in [\"f\",\"v\",\"s\",\"z\",\"sh\",\"zh\",\"th\"])\n",
    "    plos_count = sum(joined.count(p) for p in [\"p\",\"b\",\"t\",\"d\",\"k\",\"g\"])\n",
    "    vowel_count = sum(1 for c in letters if c in \"aeiouy\")\n",
    "    total = len(letters)\n",
    "    return fric_count/total, plos_count/total, vowel_count/(total+1e-9)\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import words, stopwords, wordnet as wn\n",
    "from nltk.corpus.reader import WordListCorpusReader\n",
    "import numpy as np\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Load English words and stopwords\n",
    "english_words = set(words.words())\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Define common prefixes and suffixes\n",
    "PL_PREFIXES = {\"re\", \"un\", \"in\", \"dis\", \"pre\", \"sub\"}\n",
    "PL_SUFFIXES = {\"ing\", \"ed\", \"er\", \"ly\", \"es\", \"ful\"}\n",
    "\n",
    "def is_plausible_fragment(fragment):\n",
    "    \"\"\"Check if fragment is a plausible English word, prefix/suffix, or foreign fragment.\"\"\"\n",
    "    fragment = fragment.lower()\n",
    "    if not fragment:\n",
    "        return False\n",
    "    if fragment in english_words:\n",
    "        return True\n",
    "    if fragment in PL_PREFIXES or fragment in PL_SUFFIXES:\n",
    "        return True\n",
    "    # Check if fragment exists in WordNet for any language\n",
    "    for lang in wn.langs():\n",
    "        if wn.synsets(fragment, lang=lang):\n",
    "            return True\n",
    "    # Fallback: accept fragments that are at least 2 characters long\n",
    "    if len(fragment) > 1:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def extract_audio_features_from_stanza(stanza, expected_feet_per_line=(5,6), foot_syllables=(2,3)):\n",
    "    lines = [ln.strip() for ln in stanza.strip().split(\"\\n\") if ln.strip()]\n",
    "    n_lines = max(1, len(lines))\n",
    "    tokens = tokenize_text(stanza)\n",
    "    syll_counts_tokens = [approx_syllables_word(t) for t in tokens]\n",
    "    total_syllables = sum(syll_counts_tokens)\n",
    "    n_words = len(tokens) if tokens else 1\n",
    "    syllable_density = total_syllables / n_words if n_words else 0.0\n",
    "    target_feet = np.mean(expected_feet_per_line)\n",
    "    avg_syll_per_line = total_syllables / max(1, len(lines))\n",
    "    avg_foot_syll = np.mean(foot_syllables)\n",
    "    tempo = avg_syll_per_line / avg_foot_syll\n",
    "    sylls_per_line = [sum(approx_syllables_word(t) for t in tokenize_text(ln)) for ln in lines]\n",
    "    pacing_variance = float(np.var(sylls_per_line)) if sylls_per_line else 0.0\n",
    "    fric_density, plos_density, vowel_ratio = phonetic_density(tokens)\n",
    "    vocal_smoothness = float(vowel_ratio)\n",
    "\n",
    "    # --- [word-splitting] enjambment detection (needed in this specific case; if you need to process enjambments in general see https://github.com/Margento/Computationally_Assembled_Belgian_Poetry_Anthology ---\n",
    "    enjambments = 0\n",
    "    enjambed_positions = set()\n",
    "\n",
    "    for ln_idx, ln in enumerate(lines):\n",
    "        # 1. End-of-line split (including ellipses)\n",
    "        end_match = re.search(r'(\\w+(?:\\.\\.\\.)?)-/?(\\w*)$', ln)\n",
    "        if end_match:\n",
    "            left, right = end_match.groups()\n",
    "            if is_plausible_fragment(left) and (not right or is_plausible_fragment(right)):\n",
    "                enjambments += 1\n",
    "                enjambed_positions.add(end_match.start())\n",
    "\n",
    "        # 2. Start-of-line split\n",
    "        if ln_idx > 0:\n",
    "            start_match = re.match(r'^(\\w*)-/(\\w+)', ln)\n",
    "            if start_match:\n",
    "                left, right = start_match.groups()\n",
    "                if (not left or is_plausible_fragment(left)) and is_plausible_fragment(right):\n",
    "                    enjambments += 1\n",
    "                    enjambed_positions.add(start_match.start())\n",
    "\n",
    "        # 3. Multi-word or foreign-word consideration (fallback)\n",
    "        for match in re.finditer(r'(\\S+)/(\\S+)', ln):\n",
    "            left, right = match.groups()\n",
    "            if is_plausible_fragment(left) and is_plausible_fragment(right):\n",
    "                enjambments += 1\n",
    "                enjambed_positions.add(match.start())\n",
    "\n",
    "    # Count pause marks excluding those part of valid enjambments\n",
    "    pause_marks = 0\n",
    "    for m in re.finditer(r'[,;:\\-\\—\\(\\)]', stanza):\n",
    "        if m.start() not in enjambed_positions:\n",
    "            pause_marks += 1\n",
    "\n",
    "    silence_ratio = pause_marks / (total_syllables + 1e-9)\n",
    "    caesura = sum(1 for ln in lines if \",\" in ln or \";\" in ln or \"—\" in ln)\n",
    "\n",
    "    enjambments_norm = enjambments / n_lines\n",
    "    caesura_norm = caesura / n_lines\n",
    "\n",
    "    audio = {\n",
    "        \"syllable_density\": float(syllable_density),\n",
    "        \"tempo\": float(tempo),\n",
    "        \"pacing_variance\": float(pacing_variance),\n",
    "        \"fricative_density\": float(fric_density),\n",
    "        \"plosive_density\": float(plos_density),\n",
    "        \"vocal_smoothness\": float(vocal_smoothness),\n",
    "        \"silence_ratio\": float(silence_ratio),\n",
    "        \"total_syllables\": int(total_syllables),\n",
    "        \"sylls_per_line\": sylls_per_line,\n",
    "        \"enjambments\": float(enjambments_norm),\n",
    "        \"caesura\": float(caesura_norm),\n",
    "        \"n_words\": n_words\n",
    "    }\n",
    "    return audio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a50dd53-a626-4f26-b3be-eff317542bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------------------------------------------\n",
    "# 0. Load multilingual embedding model (fast + robust)\n",
    "# --------------------------------------------------------------\n",
    "# Good models: \"sentence-transformers/distiluse-base-multilingual-cased-v2\"\n",
    "# or for higher quality: \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    "model = SentenceTransformer(\"sentence-transformers/distiluse-base-multilingual-cased-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a93e14-c684-4754-b841-adaf53cb9a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# FIRST AN EXPERIMENT ON EVENT 3 & THE WINDOW PRECEDING IT (BEFORE RUNNING THIS ON THE ENTIRE #DHSI 2023 EVENT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "733dc104-9197-4644-9b66-e63a90857b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import csr_matrix\n",
    "import networkx as nx\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- User parameters ----------\n",
    "# POEM_FOLDER = \"...r\"   # change\n",
    "# MANIFEST = \"...\"\n",
    "# POEM_SEQ = \"...\"\n",
    "# EVENT_DESC = \"....json\"\n",
    "# TWITTER_CSV = \"... synthetic_twitter_log.csv\"\n",
    "# JUPYTER_CSV = \"... synthetic_jupyter_log.csv\"\n",
    "# FACEBOOK_CSV = \"... synthetic_facebook_log.csv\"\n",
    "\n",
    "# neighborhood and window params\n",
    "# k_neigh = 10            # include ±k poems around event poems\n",
    "pre_multiplier_short = 1.0\n",
    "pre_multiplier_long = 2.0\n",
    "\n",
    "# DONE ABOVE\n",
    "# semantic/phon weights\n",
    "# alpha = 0.6\n",
    "# beta  = 0.4\n",
    "\n",
    "# ---------- load files ----------\n",
    "# with open(POEM_SEQ, \"r\") as f:\n",
    "   # poem_seq_obj = json.load(f)\n",
    "# poems = poem_seq_obj[\"poem_sequence\"]  # list of filenames\n",
    "\n",
    "# with open(EVENT_DESC, \"r\") as f:\n",
    "    # event_desc = json.load(f)\n",
    "\n",
    "# NO NEED FOR THIS IF YOU STILL HAVE THEM RUNNING IN-MEMORY\n",
    "# df_tw = pd.read_csv(TWITTER_CSV)\n",
    "# df_jh = pd.read_csv(JUPYTER_CSV)\n",
    "# df_fb = pd.read_csv(FACEBOOK_CSV)\n",
    "\n",
    "# ---------- select event 3 (3rd literally) ----------\n",
    "# event indices in event_desc are 1-based when built earlier\n",
    "event_i = 3   # third singularity\n",
    "ev = event_descriptors[event_i-1]   # descriptor for event 3\n",
    "s,e = ev[\"event_window\"]\n",
    "event_length = e - s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3aedeb-9a23-49e4-88ee-61c4aa258ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ev # SANITY CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "857ce61e-5e09-448f-988a-a26b838cf164",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "k_neigh = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1762beaa-0ffe-48e4-9277-306ab78fef60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 45 poems for subset experiment\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# poems that were saved in the event window (selected)\n",
    "poems_event_selected = ev.get(\"poems_in_window_selected\", ev.get(\"poems_in_window\", []))\n",
    "# print(\"Event 3 poems (selected):\", poems_event_selected)\n",
    "\n",
    "# ---------- build poem subset: include ±k around indices ----------\n",
    "# find indices of these poems in poems_all\n",
    "indices = [poems.index(p) for p in poems_event_selected if p in poems]\n",
    "# if none found, fall back to the 'poems_in_window' list indices\n",
    "if not indices:\n",
    "    indices = []\n",
    "    for p in ev.get(\"poems_in_window\", []):\n",
    "        if p in poems:\n",
    "            indices.append(poems.index(p))\n",
    "# expand by k_neigh\n",
    "subset_idx = set()\n",
    "for idx in indices:\n",
    "    for j in range(max(0, idx-k_neigh), min(len(poems), idx+k_neigh+1)):\n",
    "        subset_idx.add(j)\n",
    "        \n",
    "subset_idx = sorted(list(subset_idx))\n",
    "poem_subset = [poems[i] for i in subset_idx]\n",
    "print(f\"Selected {len(poem_subset)} poems for subset experiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8764e54-4811-4aa5-be4d-59e44d6246ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------- load poem texts for subset ----------\n",
    "texts = []\n",
    "valid_poems = []\n",
    "for p in poem_subset:\n",
    "    pth = os.path.join(POEM_FOLDER, p)\n",
    "    try:\n",
    "        with open(pth, \"r\", encoding=\"utf-8\") as f:\n",
    "            texts.append(f.read())\n",
    "            valid_poems.append(p)\n",
    "    except Exception as ex:\n",
    "        print(\"couldn't read\", p, ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d1f9725-6aa2-4f02-8e6e-b4d385a66e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------- compute multilingual embeddings (SBERT) ----------\n",
    "emb = model.encode(texts, convert_to_numpy=True, normalize_embeddings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e0a00f7f-32ce-47ce-a4c0-2c3c2ba38377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- compute phonological features ----------\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scalar keys returned by extract_audio_features_from_stanza\n",
    "SCALAR_KEYS = [\n",
    "    \"syllable_density\",\n",
    "    \"tempo\",\n",
    "    \"pacing_variance\",\n",
    "    \"fricative_density\",\n",
    "    \"plosive_density\",\n",
    "    \"vocal_smoothness\",\n",
    "    \"silence_ratio\",\n",
    "    \"enjambments\",\n",
    "    \"caesura\",\n",
    "    \"total_syllables\",\n",
    "    \"n_words\"\n",
    "]\n",
    "\n",
    "phon_feats = []\n",
    "\n",
    "for txt in texts:\n",
    "    feats = extract_audio_features_from_stanza(txt)\n",
    "\n",
    "    # Extract scalar features\n",
    "    vec = [float(feats[k]) for k in SCALAR_KEYS]\n",
    "\n",
    "    # Handle list-valued feature \"sylls_per_line\"\n",
    "    syll_list = feats.get(\"sylls_per_line\", [])\n",
    "    if isinstance(syll_list, list) and len(syll_list) > 0:\n",
    "        syll_mean = float(np.mean(syll_list))\n",
    "        syll_var  = float(np.var(syll_list))\n",
    "    else:\n",
    "        syll_mean = 0.0\n",
    "        syll_var  = 0.0\n",
    "\n",
    "    # Append to feature vector\n",
    "    vec.extend([syll_mean, syll_var])\n",
    "\n",
    "    phon_feats.append(vec)\n",
    "\n",
    "# Convert to numpy array\n",
    "phon_feats = np.array(phon_feats, dtype=float)\n",
    "\n",
    "# Normalize the features\n",
    "phon_feats_norm = StandardScaler().fit_transform(phon_feats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6566b3ca-2811-4c55-9640-7532183eee3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.save(\"margento_graphpoem_dhsi23_singularity_3_phonological_feats.npy\", phon_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ddaa0a5f-ad9b-4257-be3f-2d695afed81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "alpha = 0.6\n",
    "beta = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "810ff631-f15c-4d92-84b0-c3efe840335c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------- pairwise similarities ----------\n",
    "S_sem = cosine_similarity(emb)            # shape (M,M)\n",
    "S_phon = cosine_similarity(phon_feats_norm)\n",
    "S_comb = alpha * S_sem + beta * S_phon\n",
    "S_comb = np.clip(S_comb, 0.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97785d47-d4f9-4c8d-8d5a-6d31adfd3499",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a555066c-ca0a-43e4-9294-56911040ce4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------- build subset poem adjacency (sparse) ----------\n",
    "M = len(valid_poems)\n",
    "# optionally threshold or keep top-k per row\n",
    "top_k = 8\n",
    "rows=[]; cols=[]; data=[]\n",
    "for i in range(M):\n",
    "    neigh = np.argsort(-S_comb[i])[:top_k+1]\n",
    "    for j in neigh:\n",
    "        if i==j: continue\n",
    "        rows.append(i); cols.append(j); data.append(float(S_comb[i,j]))\n",
    "A_sem_subset = csr_matrix((data,(rows,cols)), shape=(M,M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "30ec1934-420b-4a91-a69c-d26e2bda38c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.save(\"margento_graphpoem_dhsi23_adjacency_matrix_singularity_3_poems.npy\", A_sem_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d604b7-1ec7-4250-aa6c-d0fb238777f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb793aa-81cc-491a-a0ff-c0acd2d1b0b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e44220fb-248b-49a1-9e3f-9d96d51d591d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------- Map social/temporal adjacency restricted to relevant users and windows ----------\n",
    "# define pre-windows\n",
    "pre_short = (int(s - pre_multiplier_short*event_length), s)\n",
    "pre_long  = (int(s - pre_multiplier_long*event_length), s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9087f17-dc23-4705-8167-236b4f050022",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ANALYZING THE SEGMENT PREVIOUS TO THE EVENT THE WAY WE ANALYZED EVENTS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5b5c9893-0c7e-49fc-89a4-73c7e7af5884",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import random\n",
    "\n",
    "previous_descriptors = []\n",
    "\n",
    "for k, prev in enumerate([pre_short, pre_long]):\n",
    "    # Twitter slice\n",
    "    tw_slice = df_tw[(df_tw[\"time\"] >= prev[0]) & (df_tw[\"time\"] <= s)]\n",
    "    num_tw_likes = int((tw_slice[\"action\"] == \"like\").sum())\n",
    "    num_tw_rts = int((tw_slice[\"action\"] == \"retweet\").sum())\n",
    "    \n",
    "    # Facebook slice\n",
    "    fb_slice = df_fb[(df_fb[\"time\"] >= prev[0]) & (df_fb[\"time\"] <= s)]\n",
    "    num_fb_likes = int((fb_slice[\"action\"] == \"like_live\").sum())\n",
    "    num_fb_shares = int((fb_slice[\"action\"] == \"share_live\").sum())\n",
    "    \n",
    "    # JupyterHub slice\n",
    "    jh_slice = df_jh[(df_jh[\"time\"] >= prev[0]) & (df_jh[\"time\"] <= s)]\n",
    "    num_jh_saves = len(jh_slice)\n",
    "    contributors = sorted(list(set(jh_slice[\"user\"].tolist())))\n",
    "    \n",
    "    # All poems in the window\n",
    "    poems_in_window = jh_slice[\"file\"].tolist()\n",
    "    \n",
    "    # Only the latest file per unique time\n",
    "    poems_in_window_selected = (\n",
    "        jh_slice.sort_values(\"time\")  # ensure sorted by time\n",
    "                .groupby(\"time\", as_index=False)  # group by time\n",
    "                .last()  # take last file for each time\n",
    "                [\"file\"]\n",
    "                .tolist()\n",
    "    )\n",
    "    \n",
    "    # Build descriptor\n",
    "    descriptor = {\n",
    "        \"prev_index\": k,\n",
    "        \"prev_window\": (prev[0], s),\n",
    "        # \"trail_window\": tr,\n",
    "        \"num_tw_likes\": num_tw_likes,\n",
    "        \"num_tw_rts\": num_tw_rts,\n",
    "        \"num_fb_likes\": num_fb_likes,\n",
    "        \"num_fb_shares\": num_fb_shares,\n",
    "        \"num_jh_saves\": num_jh_saves,\n",
    "        \"contributors\": contributors,\n",
    "        \"poems_in_window\": poems_in_window,\n",
    "        \"poems_in_window_selected\": poems_in_window_selected,\n",
    "        # \"highlighted_poems_before_during_after\": placeholder_map[ei-1],\n",
    "        \"semantic_tags\": [\"intermedia\",\"dissonance\"] if random.random() < 0.5 else [\"collage\",\"sampling\"],\n",
    "        \"media_density\": float(min(1.0, (num_tw_likes + num_tw_rts)/10.0))\n",
    "    }\n",
    "    \n",
    "    previous_descriptors.append(descriptor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "054de8e3-b558-4a19-9648-1e17c22aad37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# previous_descriptors[1] # sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "083fa7a2-4452-4ee5-981c-bf31da8e7224",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prev_path = \"margento_#graphpoem_dhsi23_windows_before_event3_descriptors.json\"\n",
    "\n",
    "with open(prev_path, \"w\") as f:\n",
    "    json.dump(previous_descriptors, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e4f66a21-d9d3-4a61-a08c-d4a83bb32d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 42 poems for subset experiment\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# poems that were saved in the event window (selected)\n",
    "prev_0_poems_selected = previous_descriptors[0].get(\"poems_in_window_selected\", previous_descriptors[0].get(\"poems_in_window\", []))\n",
    "# print(\"Short window before event 3 poems (selected):\", prev_0_poems_selected)\n",
    "\n",
    "# ---------- build poem subset: include ±k around indices ----------\n",
    "# find indices of these poems in poems_all\n",
    "indices = [poems.index(p) for p in prev_0_poems_selected if p in poems]\n",
    "# if none found, fall back to the 'poems_in_window' list indices\n",
    "if not indices:\n",
    "    indices = []\n",
    "    for p in previous_descriptors[0].get(\"poems_in_window\", []):\n",
    "        if p in poems:\n",
    "            indices.append(poems.index(p))\n",
    "# expand by k_neigh\n",
    "subset_idx = set()\n",
    "for idx in indices:\n",
    "    for j in range(max(0, idx-k_neigh), min(len(poems), idx+k_neigh+1)):\n",
    "        subset_idx.add(j)\n",
    "        \n",
    "subset_idx = sorted(list(subset_idx))\n",
    "poem_subset = [poems[i] for i in subset_idx]\n",
    "print(f\"Selected {len(poem_subset)} poems for subset experiment\")\n",
    "\n",
    "# ---------- load poem texts for subset ----------\n",
    "texts_prev = []\n",
    "valid_poems_prev = []\n",
    "\n",
    "for p in poem_subset:\n",
    "    pth = os.path.join(POEM_FOLDER, p)\n",
    "    try:\n",
    "        with open(pth, \"r\", encoding=\"utf-8\") as f:\n",
    "            texts_prev.append(f.read())\n",
    "            valid_poems_prev.append(p)\n",
    "    except Exception as ex:\n",
    "        print(\"couldn't read\", p, ex)\n",
    "\n",
    "# ---------- embeddings ----------\n",
    "emb_prev = model.encode(texts_prev, convert_to_numpy=True, normalize_embeddings=True)\n",
    "\n",
    "# ---------- compute phonological features ----------\n",
    "\n",
    "SCALAR_KEYS = [\n",
    "    \"syllable_density\",\n",
    "    \"tempo\",\n",
    "    \"pacing_variance\",\n",
    "    \"fricative_density\",\n",
    "    \"plosive_density\",\n",
    "    \"vocal_smoothness\",\n",
    "    \"silence_ratio\",\n",
    "    \"enjambments\",\n",
    "    \"caesura\",\n",
    "    \"total_syllables\",\n",
    "    \"n_words\"\n",
    "]\n",
    "\n",
    "phon_feats_prev = []\n",
    "\n",
    "for txt in texts_prev:\n",
    "    feats = extract_audio_features_from_stanza(txt)\n",
    "\n",
    "    # Extract scalar features\n",
    "    vec = [float(feats[k]) for k in SCALAR_KEYS]\n",
    "\n",
    "    # Handle list-valued feature \"sylls_per_line\"\n",
    "    syll_list = feats.get(\"sylls_per_line\", [])\n",
    "    if isinstance(syll_list, list) and len(syll_list) > 0:\n",
    "        syll_mean = float(np.mean(syll_list))\n",
    "        syll_var  = float(np.var(syll_list))\n",
    "    else:\n",
    "        syll_mean = 0.0\n",
    "        syll_var  = 0.0\n",
    "\n",
    "    # Append to feature vector\n",
    "    vec.extend([syll_mean, syll_var])\n",
    "\n",
    "    phon_feats_prev.append(vec)\n",
    "\n",
    "# Convert to numpy array\n",
    "phon_feats_prev = np.array(phon_feats_prev, dtype=float)\n",
    "\n",
    "# Normalize the features\n",
    "phon_feats_norm_prev = StandardScaler().fit_transform(phon_feats_prev)\n",
    "\n",
    "np.save(\"margento_graphpoem_dhsi23_wondow_previous_to_singularity_3_phonological_feats.npy\", phon_feats_prev)\n",
    "\n",
    "# ---------- pairwise similarities ----------\n",
    "S_sem_prev = cosine_similarity(emb_prev)            # shape (M_prev,M_prev)\n",
    "S_phon_prev = cosine_similarity(phon_feats_norm_prev)\n",
    "S_comb_prev = alpha * S_sem_prev + beta * S_phon_prev\n",
    "S_comb_prev = np.clip(S_comb_prev, 0.0, 1.0)\n",
    "\n",
    "# ---------- build subset poem adjacency (sparse) for window previous to event 3 ----------\n",
    "M_prev = len(valid_poems_prev)\n",
    "\n",
    "# optionally threshold or keep top-k per row\n",
    "top_k = 8\n",
    "rows=[]; cols=[]; data=[]\n",
    "\n",
    "for i in range(M_prev):\n",
    "    neigh = np.argsort(-S_comb_prev[i])[:top_k+1]\n",
    "    for j in neigh:\n",
    "        if i==j: continue\n",
    "        rows.append(i); cols.append(j); data.append(float(S_comb_prev[i,j]))\n",
    "\n",
    "A_sem_subset_prev = csr_matrix((data,(rows,cols)), shape=(M_prev,M_prev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0a11737c-bf14-4749-9775-3bf58213659a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"margento_graphpoem_dhsi23_semantic_n_sonic_matrix_window_previous_to_singularity_3.npy\", A_sem_subset_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0fb7409b-13f5-4b4b-98ee-bbf6efa5d70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "intersection_prev_n_event = set(prev_0_poems_selected).intersection(set(poems_event_selected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "72d43200-5c93-40ad-acaa-936eeaf934b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(intersection_prev_n_event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccc0ca2-514c-4461-a539-aaeae45b5c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ONLY ONE POEM SHARED BY THE TWO CHUNKS (PREVIOUS WINDOW AND EVENT)!!!!!!!!!!\n",
    "# THERE ARE OTHER POEMS SHARED BY THE TWO CHUNKS IN GENERAL\n",
    "# BUT THIS IS THE ONLY ONE IN THE INTERSECTION OF THE SELECTED POEMS\n",
    "# THAT IS, POEMS FOR EACH CHUNK OUTPUTTED BY (THE LIVE INTERACTIVE CODING ON) JUPYTERHUB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "66f12b98-18da-4903-9ee7-255de201025f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(prev_0_poems_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c443be3e-2fac-4095-8d09-868c4144c820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(poems_event_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced38c3d-3a9a-4bcb-b275-cee47f3e6fd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2b04d0a2-0d78-4624-91f4-dbe5e2f78d4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'place_rachel_blau_duplessis_draft_65_that_japanese_language_tao.txt'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intersection_prev_n_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f6b40440-9cd1-4a21-ac30-21a5a518913e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "intersection_prev_n_event_valid = set(valid_poems_prev).intersection(set(valid_poems))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8dc10b61-9b45-413d-ad3b-f6d2f681ce6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(intersection_prev_n_event_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0af86dab-786c-475a-b1ec-935daa04512a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'place_oliver_goldsmith_france.txt',\n",
       " 'place_rachel_blau_duplessis_draft_65_that_japanese_language_tao.txt',\n",
       " 'place_rachel_by_matthew_arnold_paris_provence_french_riviera.txt',\n",
       " 'qiu_jin_pusaman_a_message_for_a_female_friend_from_chinese_trans_yilin_wang_poem_1.txt',\n",
       " 'qiu_jin_reflections_from_chinese_trans_yilin_wang_poem_3.txt',\n",
       " 'sagawa_chika_dark_song_from_japanese_trans_sawako_nakayasu_poem_2.txt',\n",
       " 'sagawa_chika_ocean_angel_from_japanese_trans_sawako_nakayasu_poem_3.txt',\n",
       " 'satu-taskinen_bio.txt',\n",
       " 'satu-taskinen_interview.txt'}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intersection_prev_n_event_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34804829-ec85-4362-88bc-ff85aa60c773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE ABOVE ARE THE INTERSECTION BETWEEN THE EXPANDED SETS OF POEMS PER WINDOW \n",
    "# (INCL. POEMS WITHIN THE K_NEIGH WINDOW IN THE CORPUS AROUND EACH POEM IN THE WINDOW) \n",
    "# WHEREAS THE UNIQUE ONE PREVIOUSLY IDENTIFIED IS THE INTERSECTION OF THOSE OUTPUTTED BY JUPYTERHUB\n",
    "# THAT IS, THE VALUES OF THE \"poems_in_window_selected\" KEYS IN BOTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1f5cce37-03df-4e89-adf3-0d39ee38e6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "intersection_prev_n_event_users = set(previous_descriptors[0][\"contributors\"]).intersection(set(ev[\"contributors\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "46994a86-2bea-4887-8eba-4cf4c0e235c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(intersection_prev_n_event_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4b978673-9a2d-47cf-abd1-531f810e4efd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(set(previous_descriptors[0][\"contributors\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "62c15d07-2699-4ed5-931b-a50597767c96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(ev[\"contributors\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78548a3b-38e2-4a16-87ea-0b415e0f861b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LET'S ANALYZE THAT SINGLE SHARED POEM AND HOW ITS TOPOLOGICAL FEATURES SHIFT AS IT TRANSITIONS FROM PRE-WINDOW TO EVENT PER SE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a3c3638f-1a13-43e6-840d-2da22c3174e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. IDENTIFY THE SHARED POEM--WE ALREADY DID THAT, NOW WE ARE GETTING ITS INDICES IN BOTH WORLDS\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# shared_poems = list(set(valid_poems_prev) & set(valid_poems))\n",
    "# if len(shared_poems) != 1:\n",
    "    # print(\"Warning: expected exactly one shared poem, found:\", shared_poems)\n",
    "\n",
    "# p_shared = shared_poems[0]\n",
    "# print(\"Shared poem:\", p_shared)\n",
    "p_shared = list(intersection_prev_n_event)[0]\n",
    "\n",
    "# indices inside each window\n",
    "i_prev = valid_poems_prev.index(p_shared)\n",
    "i_evt  = valid_poems.index(p_shared)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "35381212-d8bc-4e1e-95a0-b9e0cb3e968a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 13\n"
     ]
    }
   ],
   "source": [
    "print(i_prev, i_evt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4b46c240-3ed6-4d29-ad2f-2d3962d0fb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic shift ‖emb_prev - emb_evt‖: 0.0\n",
      "Phonological shift ‖phon_prev - phon_evt‖: 0.0\n",
      "Neighbor overlap: 1\n",
      "Neighbors PRE: ['osip_mandelstam_rome_from_russian_trans_john_high_and_matvei_yankelevich_poem_1.txt', 'place_lupe-gomez_trans-erin-moure_secret-energy_hermida-galicia-&-germany.txt', 'place_rachel_by_matthew_arnold_paris_provence_french_riviera.txt', 'osip_mandelstam_meganom_from_russian_trans_alistair_noon_poem_1.txt', 'paulo_leminski_reportedly_from_portuguese_trans_elisa_wouk_almino_poem_4.txt', 'place_oliver_goldsmith_france.txt', 'place_margento_tower_mask_cannes_france.txt', 'osip_mandelstam_january_9_1937_from_russian_trans_john_high_and_matvei_yankelevich_poem_2.txt']\n",
      "Neighbors EVENT: ['pierre_peuchmaurd_glimmers_from_french_trans_ec_belli_poem_1.txt', 'saksiri_meesomsueb_dogs_in_the_lead_from_thai_trans_noh_anothai_poem_3.txt', 'qiu_jin_reflections_from_chinese_trans_yilin_wang_poem_3.txt', 'phu_recalling_love_scenes_by_pleasant_river_from_thai_trans_noh_anothai_poem_2.txt', 'pierre_peuchmaurd_the_foam_of_lions_from_french_trans_ec_belli_poem_2.txt', 'saksiri_meesomsueb_sleight_from_thai_trans_noh_anothai_poem_1.txt', 'sappho_31_trans_julia_dubnoff_trans_chris_childers_trans_anne_carson_walt_whitman_woman_waits_for_me.txt', 'place_rachel_by_matthew_arnold_paris_provence_french_riviera.txt']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# SEMANTIC SHIFT\n",
    "emb_prev_vec = emb_prev[i_prev]\n",
    "emb_evt_vec  = emb[i_evt]\n",
    "\n",
    "semantic_shift = float(np.linalg.norm(emb_prev_vec - emb_evt_vec))\n",
    "print(\"Semantic shift ‖emb_prev - emb_evt‖:\", semantic_shift)\n",
    "\n",
    "# SONIC SHIFT\n",
    "phon_prev_vec = phon_feats_prev[i_prev]\n",
    "phon_evt_vec  = phon_feats[i_evt]\n",
    "\n",
    "phonological_shift = float(np.linalg.norm(phon_prev_vec - phon_evt_vec))\n",
    "print(\"Phonological shift ‖phon_prev - phon_evt‖:\", phonological_shift)\n",
    "\n",
    "# NEIGHBORHOOD SHIFT\n",
    "def top_neighbors(A, i, k=8):\n",
    "    row = A[i].toarray().flatten()\n",
    "    idx = np.argsort(-row)\n",
    "    idx = [j for j in idx if j != i][:k]\n",
    "    return idx\n",
    "\n",
    "N_prev = top_neighbors(A_sem_subset_prev, i_prev)\n",
    "N_evt  = top_neighbors(A_sem_subset, i_evt)\n",
    "\n",
    "# convert neighbor indices to poem names\n",
    "N_prev_names = [valid_poems_prev[j] for j in N_prev]\n",
    "N_evt_names  = [valid_poems[j] for j in N_evt]\n",
    "\n",
    "overlap = len(set(N_prev_names) & set(N_evt_names))\n",
    "print(\"Neighbor overlap:\", overlap)\n",
    "print(\"Neighbors PRE:\", N_prev_names)\n",
    "print(\"Neighbors EVENT:\", N_evt_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c636c584-1a8c-448c-9f5c-07fa18a9293f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'place_rachel_by_matthew_arnold_paris_provence_french_riviera.txt'}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "set(N_prev_names) & set(N_evt_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dedc38a3-6f3b-4f89-bcb8-cad4a27e38ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbor overlap: 8\n",
      "Neighbors PRE: ['osip_mandelstam_rome_from_russian_trans_john_high_and_matvei_yankelevich_poem_1.txt', 'place_lupe-gomez_trans-erin-moure_secret-energy_hermida-galicia-&-germany.txt', 'place_rachel_by_matthew_arnold_paris_provence_french_riviera.txt', 'osip_mandelstam_meganom_from_russian_trans_alistair_noon_poem_1.txt', 'paulo_leminski_reportedly_from_portuguese_trans_elisa_wouk_almino_poem_4.txt', 'place_oliver_goldsmith_france.txt', 'place_margento_tower_mask_cannes_france.txt', 'osip_mandelstam_january_9_1937_from_russian_trans_john_high_and_matvei_yankelevich_poem_2.txt', 'qiu_jin_to_drink_from_chinese_trans_yilin_wang_poem_5.txt', 'rosa_chávez_i_braid_my_hair_so_it_rests_from_spanish_trans_gabriela_ramirezchavez_poem_4.txt', 'rosa_chávez_in_our_palm_lines_our_tenderness_is_written_from_spanish_trans_gabriela_ramirezchavez_poem_6.txt', 'rosa_chávez_may_my_heart_bloom_when_it_stops_pumping_red_ink_may_it_burst_into_small_thorns_from_spanish_trans_gabriela_ramirezchavez_poem_9.txt', 'rose_ausländer_unity_of_dust_from_german_trans_carlie_hoffman_poem_3.txt', 'roselyne_sibille_i_write_to_you_from_the_other_side_of_the_shadow_from_french_trans_karthika_naïr_poem_1.txt', 'roselyne_sibille_barbwire_in_your_mouth_from_french_trans_karthika_naïr_poem_4.txt', 'qiu_jin_reflections_from_chinese_trans_yilin_wang_poem_3.txt', 'roselyne_sibille_latitudes_flee_from_french_trans_karthika_naïr_poem_2.txt', 'roselyne_sibille_the_shadow_moans_from_french_trans_karthika_naïr_poem_3.txt', 'sagawa_chika_afternoon_from_japanese_trans_sawako_nakayasu_poem_1.txt', 'sagawa_chika_dark_song_from_japanese_trans_sawako_nakayasu_poem_2.txt', 'sagawa_chika_ocean_angel_from_japanese_trans_sawako_nakayasu_poem_3.txt', 'satu-taskinen_bio.txt', 'rose_ausländer_without_a_visa_from_german_trans_carlie_hoffman_poem_2.txt', 'qiu_jin_pusaman_a_message_for_a_female_friend_from_chinese_trans_yilin_wang_poem_1.txt', 'ondrej-buddeus_chatroom-the-history-of-the-coca-cola-trademark-in-central-and-eastern-europe-from-1990-to-2011.txt', 'publishing_sphere_reader_lionel_ruffel_contemporary_moment.txt', 'oriette_dangelo_caracas_in_your_absence_i_know_ill_find_my_name_from_spanish_trans_lupita_eydetucker_poem_1.txt', 'oriette_dangelo_criminal_syndrome_from_spanish_trans_lupita_eydetucker_poem_5.txt', 'paalhelge_haugen_61_from_norwegian_trans_julia_johanne_tolo_poem_5.txt', 'paalhelge_haugen_65_from_norwegian_trans_julia_johanne_tolo_poem_6.txt', 'paalhelge_haugen_67_from_norwegian_trans_julia_johanne_tolo_poem_7.txt', 'patron_henekou_letter_xv_to_the_spokesperson_of_parliament_from_french_trans_patron_henekou_poem_2.txt', 'publishing_sphere_reader_nick_thurston_subcontract.txt', 'paulo_leminski_sets_versus_setbacks_from_portuguese_trans_elisa_wouk_almino_poem_2.txt', 'place_abraham-sutzkever_trans-seymour-mayne_poem-about-a-herring_thrid-reich-&-warsaw-&-poland.txt', 'place_alexis-almeida_something-tells-you-to-look.txt', \"place_martin-glaz-serup_roman-nights_rome-venice-colle-di-val-d'elsa-italy-&-nicaragua-&-russia-&-usa-&-amager-denmark-&-sarajevo-bosnia-&-costa-brava-spain-&-gothenburg-sweden-&-berlin-germany.txt\", 'satu-taskinen_interview.txt', 'publishing_sphere_reader_erin_moure_reading_translation_multilingual_poetry.txt', 'piyush_daiya_9_from_hindi_trans_rahul_soni_poem_4.txt', 'sayaka_osaki_momo_in_particular_the_scenes_in_which_the_tortoise_cassiopeia_appears_authors_note_hrefaboo_from_japanese_trans_jeffrey_angles_poem_1.txt']\n",
      "Neighbors EVENT: ['pierre_peuchmaurd_glimmers_from_french_trans_ec_belli_poem_1.txt', 'saksiri_meesomsueb_dogs_in_the_lead_from_thai_trans_noh_anothai_poem_3.txt', 'qiu_jin_reflections_from_chinese_trans_yilin_wang_poem_3.txt', 'phu_recalling_love_scenes_by_pleasant_river_from_thai_trans_noh_anothai_poem_2.txt', 'pierre_peuchmaurd_the_foam_of_lions_from_french_trans_ec_belli_poem_2.txt', 'saksiri_meesomsueb_sleight_from_thai_trans_noh_anothai_poem_1.txt', 'sappho_31_trans_julia_dubnoff_trans_chris_childers_trans_anne_carson_walt_whitman_woman_waits_for_me.txt', 'place_rachel_by_matthew_arnold_paris_provence_french_riviera.txt', 'raúl_zurita_felipe_zurita_from_spanish_trans_daniel_borzutzky_poem_3.txt', 'raúl_zurita_iván_zurita_from_spanish_trans_daniel_borzutzky_poem_2.txt', 'raúl_zurita_josefina_pessolo_from_spanish_trans_daniel_borzutzky_poem_5.txt', 'sagawa_chika_dark_song_from_japanese_trans_sawako_nakayasu_poem_2.txt', 'sagawa_chika_ocean_angel_from_japanese_trans_sawako_nakayasu_poem_3.txt', 'oriette_dangelo_too_much_bone_from_spanish_trans_lupita_eydetucker_poem_2.txt', 'rasool_yoonan_untitled_1_from_persian_trans_siavash_saadlou_poem_1.txt', 'salaiman_juhni_roots_from_arabic_trans_ali_kadhim_and_chris_george_poem_3.txt', 'satu-taskinen_bio.txt', 'satu-taskinen_interview.txt', 'seo_jung_hak_paper_box_from_korean_trans_megan_sungyoon_poem_4.txt', 'seo_jung_hak_surreal_machinery_and_green_trees_from_korean_trans_megan_sungyoon_poem_3.txt', 'seo_jung_hak_the_cheapest_france_in_town_from_korean_trans_megan_sungyoon_poem_6.txt', 'shanzhai_comedy.txt', 'saksiri_meesomsueb_doze_from_thai_trans_noh_anothai_poem_2.txt', 'rachida_madani_xiii_from_french_trans_marilyn_hacker_poem_4.txt', 'quyên_nguyễnhoàng_a_poetics_of_sleep_2_the_no_center_of_writing_from_vietnamese_trans_quyên_nguyễnhoàng_poem_2.txt', 'quyên_nguyễnhoàng_a_poetics_of_sleep_3_a_river_secretes_many_mouths_from_vietnamese_trans_quyên_nguyễnhoàng_poem_1.txt', 'oriette_dangelo_too_much_calcium_from_spanish_trans_lupita_eydetucker_poem_3.txt', 'osdany_morales_what_is_your_favorite_movienbsp_from_spanish_trans_harry_bauld_poem_1.txt', 'paalhelge_haugen_77_from_norwegian_trans_julia_johanne_tolo_poem_10.txt', 'pablo_ingberg_smoke_signals_from_spanish_trans_sarah_moses_poem_2.txt', 'pablo_ingberg_the_scene_of_the_crime_from_spanish_trans_sarah_moses_poem_3.txt', 'pamela_proietti_pinhole_from_italian_trans_donna_mancusiungaro_hart_and_stephen_eric_berry_poem_1.txt', 'pamela_proietti_sicily_from_italian_trans_donna_mancusiungaro_hart_and_stephen_eric_berry_poem_3.txt', 'pamela_proietti_tanka_n_36_from_italian_trans_donna_mancusiungaro_hart_and_stephen_eric_berry_poem_4.txt', 'place_oliver_goldsmith_france.txt', 'publishing_sphere_danny_snelson_heath_tracing_actor_as_network.txt', 'publishing_sphere_designabilities_blog_post.txt', 'publishing_sphere_diana_hamilton_autobiography_of_fatness.txt', 'qiu_jin_inscriptions_on_my_tiny_portrait_in_mens_clothes_from_chinese_trans_yilin_wang_poem_2.txt', 'qiu_jin_pusaman_a_message_for_a_female_friend_from_chinese_trans_yilin_wang_poem_1.txt', 'quotes_tempest_dr_aidan.txt', 'shanzhai_darling.txt', 'rachida_madani_xi_from_french_trans_marilyn_hacker_poem_3.txt', 'shanzhai_delighted.txt']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ALL NEIGHBORS\n",
    "def neighbors(A, i):\n",
    "    row = A[i].toarray().flatten()\n",
    "    idx = np.argsort(-row)\n",
    "    # idx = [j for j in idx if j != i][:k]\n",
    "    idx = [j for j in idx if j != i]\n",
    "    return idx\n",
    "\n",
    "N_prev = neighbors(A_sem_subset_prev, i_prev)\n",
    "N_evt  = neighbors(A_sem_subset, i_evt)\n",
    "\n",
    "# convert neighbor indices to poem names\n",
    "N_prev_names = [valid_poems_prev[j] for j in N_prev]\n",
    "N_evt_names  = [valid_poems[j] for j in N_evt]\n",
    "\n",
    "overlap = len(set(N_prev_names) & set(N_evt_names))\n",
    "print(\"Neighbor overlap:\", overlap)\n",
    "print(\"Neighbors PRE:\", N_prev_names)\n",
    "print(\"Neighbors EVENT:\", N_evt_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c5f9d1fd-6e5b-4d31-a033-7656ef695f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap = set(N_prev_names) & set(N_evt_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ab01275e-3720-43f8-b579-099b4afa22d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'place_oliver_goldsmith_france.txt',\n",
       " 'place_rachel_by_matthew_arnold_paris_provence_french_riviera.txt',\n",
       " 'qiu_jin_pusaman_a_message_for_a_female_friend_from_chinese_trans_yilin_wang_poem_1.txt',\n",
       " 'qiu_jin_reflections_from_chinese_trans_yilin_wang_poem_3.txt',\n",
       " 'sagawa_chika_dark_song_from_japanese_trans_sawako_nakayasu_poem_2.txt',\n",
       " 'sagawa_chika_ocean_angel_from_japanese_trans_sawako_nakayasu_poem_3.txt',\n",
       " 'satu-taskinen_bio.txt',\n",
       " 'satu-taskinen_interview.txt'}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8eedc4-35b3-4592-9939-ea1928b8ee26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a95a4915-98aa-44ae-8e2e-4fd3190f30f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('margento_graphpoem_dhsi23_valid_poems_window_prev_to_singualrity_3.pkl', 'wb') as file:\n",
    "    pickle.dump(valid_poems_prev, file)\n",
    "\n",
    "with open('margento_graphpoem_dhsi23_valid_poems_singualrity_3.pkl', 'wb') as file0:\n",
    "    pickle.dump(valid_poems, file0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f9241255-1ca7-4cc1-90bf-1eb0ead0754b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted degree PRE: 3.2140706944756348\n",
      "Weighted degree EVENT: 4.323058051327843\n",
      "Δ weighted degree: 1.1089873568522082\n",
      "Closeness PRE: 0.17569417710631507\n",
      "Closeness EVENT: 0.18217927159944716\n",
      "Δ closeness: 0.0064850944931320875\n",
      "\n",
      "Betweenness PRE: 0.10121951219512196\n",
      "Betweenness EVENT: 0.007399577167019028\n",
      "Δ betweenness: -0.09381993502810293\n",
      "\n",
      "Eigenvector centrality PRE: 0.032616880487014256\n",
      "Eigenvector centrality EVENT: 0.19246052431567587\n",
      "Δ eigenvector: 0.15984364382866162\n",
      "\n",
      "PageRank PRE: 0.019632008573655848\n",
      "PageRank EVENT: 0.019913861636161517\n",
      "Δ PageRank: 0.00028185306250566936\n",
      "\n",
      "Clustering PRE: 0.21065843866034695\n",
      "Clustering EVENT: 0.353299341125147\n",
      "Δ clustering: 0.14264090246480005\n",
      "\n",
      "Num neighbors PRE: 11\n",
      "Num neighbors EVENT: 10\n",
      "Δ neighbors: -1\n",
      "\n",
      "Mean edge weight PRE: 0.2921882449523304\n",
      "Mean edge weight EVENT: 0.4323058051327844\n",
      "Median PRE: 0.30983276841097257\n",
      "Median EVENT: 0.43125422227755356\n",
      "Δ mean weight: 0.14011756018045396\n",
      "\n",
      "Louvain library not installed; skip community detection.\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "# Convert your sparse matrices to networkx graphs\n",
    "G_prev = nx.from_scipy_sparse_array(A_sem_subset_prev, create_using=nx.Graph)\n",
    "G_evt  = nx.from_scipy_sparse_array(A_sem_subset, create_using=nx.Graph)\n",
    "\n",
    "# ALREADY DONE BUT RUN AGAIN IF NEW SESSION\n",
    "# p = p_shared\n",
    "# i_prev = valid_poems_prev.index(p)\n",
    "# i_evt  = valid_poems.index(p)\n",
    "\n",
    "# Mapping from index to poem name for printing\n",
    "mapping_prev = {i:valid_poems_prev[i] for i in range(len(valid_poems_prev))}\n",
    "mapping_evt  = {i:valid_poems[i]  for i in range(len(valid_poems))}\n",
    "\n",
    "# -------------------------------\n",
    "# 1. DEGREE CENTRALITY\n",
    "# -------------------------------\n",
    "deg_prev = G_prev.degree(i_prev, weight='weight')\n",
    "deg_evt  = G_evt.degree(i_evt,  weight='weight')\n",
    "\n",
    "print(\"Weighted degree PRE:\", deg_prev)\n",
    "print(\"Weighted degree EVENT:\", deg_evt)\n",
    "print(\"Δ weighted degree:\", deg_evt - deg_prev)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. CLOSENESS CENTRALITY\n",
    "# -------------------------------\n",
    "\n",
    "# Build a distance-weighted version of the graphs\n",
    "G_prev_dist = G_prev.copy()\n",
    "G_evt_dist  = G_evt.copy()\n",
    "\n",
    "for u, v, d in G_prev_dist.edges(data=True):\n",
    "    w = d.get('weight', 1e-9)\n",
    "    d['weight'] = 1.0 / max(w, 1e-9)\n",
    "\n",
    "for u, v, d in G_evt_dist.edges(data=True):\n",
    "    w = d.get('weight', 1e-9)\n",
    "    d['weight'] = 1.0 / max(w, 1e-9)\n",
    "\n",
    "cl_prev = nx.closeness_centrality(G_prev_dist, i_prev, distance='weight')\n",
    "cl_evt  = nx.closeness_centrality(G_evt_dist,  i_evt,  distance='weight')\n",
    "\n",
    "print(\"Closeness PRE:\", cl_prev)\n",
    "print(\"Closeness EVENT:\", cl_evt)\n",
    "print(\"Δ closeness:\", cl_evt - cl_prev)\n",
    "\n",
    "# -------------------------------\n",
    "# 3. BETWEENNESS CENTRALITY\n",
    "# -------------------------------\n",
    "# (run weighted betweenness)\n",
    "bt_prev = nx.betweenness_centrality(G_prev, weight='weight')[i_prev]\n",
    "bt_evt  = nx.betweenness_centrality(G_evt,  weight='weight')[i_evt]\n",
    "\n",
    "print(\"\\nBetweenness PRE:\", bt_prev)\n",
    "print(\"Betweenness EVENT:\", bt_evt)\n",
    "print(\"Δ betweenness:\", bt_evt - bt_prev)\n",
    "\n",
    "# -------------------------------\n",
    "# 4. EIGENVECTOR CENTRALITY\n",
    "# -------------------------------\n",
    "eig_prev = nx.eigenvector_centrality_numpy(G_prev, weight='weight')[i_prev]\n",
    "eig_evt  = nx.eigenvector_centrality_numpy(G_evt,  weight='weight')[i_evt]\n",
    "\n",
    "print(\"\\nEigenvector centrality PRE:\", eig_prev)\n",
    "print(\"Eigenvector centrality EVENT:\", eig_evt)\n",
    "print(\"Δ eigenvector:\", eig_evt - eig_prev)\n",
    "\n",
    "# -------------------------------\n",
    "# 5. PAGERANK\n",
    "# -------------------------------\n",
    "pr_prev_dict = nx.pagerank(G_prev, weight='weight')\n",
    "pr_evt_dict  = nx.pagerank(G_evt,  weight='weight')\n",
    "\n",
    "pr_prev = pr_prev_dict.get(i_prev, 0.0)\n",
    "pr_evt  = pr_evt_dict.get(i_evt, 0.0)\n",
    "\n",
    "print(\"\\nPageRank PRE:\", pr_prev)\n",
    "print(\"PageRank EVENT:\", pr_evt)\n",
    "print(\"Δ PageRank:\", pr_evt - pr_prev)\n",
    "\n",
    "# -------------------------------\n",
    "# 6. LOCAL CLUSTERING COEFFICIENT\n",
    "# -------------------------------\n",
    "cc_prev = nx.clustering(G_prev, i_prev, weight='weight')\n",
    "cc_evt  = nx.clustering(G_evt,  i_evt,  weight='weight')\n",
    "\n",
    "print(\"\\nClustering PRE:\", cc_prev)\n",
    "print(\"Clustering EVENT:\", cc_evt)\n",
    "print(\"Δ clustering:\", cc_evt - cc_prev)\n",
    "\n",
    "# -------------------------------\n",
    "# 7. NEIGHBORHOOD GROWTH / SHRINK\n",
    "# -------------------------------\n",
    "N_prev = list(G_prev.neighbors(i_prev))\n",
    "N_evt  = list(G_evt.neighbors(i_evt))\n",
    "\n",
    "print(\"\\nNum neighbors PRE:\", len(N_prev))\n",
    "print(\"Num neighbors EVENT:\", len(N_evt))\n",
    "print(\"Δ neighbors:\", len(N_evt) - len(N_prev))\n",
    "\n",
    "# -------------------------------\n",
    "# 8. EDGE WEIGHT DISTRIBUTION\n",
    "# -------------------------------\n",
    "weights_prev = [G_prev[i_prev][nbr]['weight'] for nbr in N_prev]\n",
    "weights_evt  = [G_evt[i_evt][nbr]['weight']  for nbr in N_evt]\n",
    "\n",
    "print(\"\\nMean edge weight PRE:\", np.mean(weights_prev))\n",
    "print(\"Mean edge weight EVENT:\", np.mean(weights_evt))\n",
    "print(\"Median PRE:\", np.median(weights_prev))\n",
    "print(\"Median EVENT:\", np.median(weights_evt))\n",
    "print(\"Δ mean weight:\", np.mean(weights_evt) - np.mean(weights_prev))\n",
    "\n",
    "# -------------------------------\n",
    "# 9. COMMUNITY ROLE (Louvain)\n",
    "# -------------------------------\n",
    "try:\n",
    "    import community as community_louvain\n",
    "\n",
    "    com_prev = community_louvain.best_partition(G_prev, weight='weight')\n",
    "    com_evt  = community_louvain.best_partition(G_evt,  weight='weight')\n",
    "\n",
    "    print(\"\\nCommunity PRE:\", com_prev[i_prev])\n",
    "    print(\"Community EVENT:\", com_evt[i_evt])\n",
    "    print(\"Community changed:\", com_prev[i_prev] != com_evt[i_evt])\n",
    "except:\n",
    "    print(\"\\nLouvain library not installed; skip community detection.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "55375f7d-1cef-4417-a89b-58225d567e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from networkx.algorithms.community import louvain_communities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e345e7ee-1b06-44af-b518-119e0525e5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shared poem community PRE: 1\n",
      "Shared poem community EVENT: 2\n",
      "Δ community (if changed): True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- detect communities ---\n",
    "com_prev = louvain_communities(G_prev, weight=\"weight\")\n",
    "com_evt  = louvain_communities(G_evt,  weight=\"weight\")\n",
    "\n",
    "def communities_to_partition(communities):\n",
    "    part = {}\n",
    "    for cid, comm in enumerate(communities):\n",
    "        for node in comm:\n",
    "            part[node] = cid\n",
    "    return part\n",
    "\n",
    "part_prev = communities_to_partition(com_prev)\n",
    "part_evt  = communities_to_partition(com_evt)\n",
    "\n",
    "# --- community of the shared poem ---\n",
    "c_prev = part_prev.get(i_prev, None)\n",
    "c_evt  = part_evt.get(i_evt, None)\n",
    "\n",
    "print(\"Shared poem community PRE:\", c_prev)\n",
    "print(\"Shared poem community EVENT:\", c_evt)\n",
    "print(\"Δ community (if changed):\", c_evt != c_prev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498a720c-cc63-45ae-9b2d-1d367999ff26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "09069b6a-d6dc-4bea-b4f7-bd5e3a00f8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 13\n"
     ]
    }
   ],
   "source": [
    "print(i_prev, i_evt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "272398fe-2e4a-4a19-bb84-f9411b44526c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(com_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5f01698b-3073-4c55-ad8d-28595455c1fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(com_evt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0c4f20dc-67fe-4680-97e9-2296ee27d604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0,\n",
       " 1: 0,\n",
       " 2: 0,\n",
       " 39: 0,\n",
       " 40: 0,\n",
       " 15: 0,\n",
       " 17: 0,\n",
       " 21: 0,\n",
       " 22: 0,\n",
       " 23: 0,\n",
       " 3: 1,\n",
       " 4: 1,\n",
       " 5: 1,\n",
       " 36: 1,\n",
       " 10: 1,\n",
       " 16: 1,\n",
       " 18: 1,\n",
       " 19: 1,\n",
       " 20: 1,\n",
       " 24: 1,\n",
       " 25: 1,\n",
       " 26: 1,\n",
       " 28: 1,\n",
       " 29: 1,\n",
       " 6: 2,\n",
       " 7: 2,\n",
       " 8: 2,\n",
       " 9: 2,\n",
       " 11: 2,\n",
       " 12: 2,\n",
       " 13: 2,\n",
       " 14: 2,\n",
       " 27: 2,\n",
       " 30: 2,\n",
       " 31: 2,\n",
       " 32: 2,\n",
       " 33: 2,\n",
       " 34: 2,\n",
       " 35: 2,\n",
       " 37: 2,\n",
       " 38: 2,\n",
       " 41: 2}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "part_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9660fa79-e806-4955-b75b-3dd8e9ed8422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0,\n",
       " 35: 0,\n",
       " 37: 0,\n",
       " 38: 0,\n",
       " 39: 0,\n",
       " 40: 0,\n",
       " 41: 0,\n",
       " 15: 0,\n",
       " 16: 0,\n",
       " 22: 0,\n",
       " 0: 1,\n",
       " 2: 1,\n",
       " 4: 1,\n",
       " 5: 1,\n",
       " 6: 1,\n",
       " 7: 1,\n",
       " 8: 1,\n",
       " 21: 1,\n",
       " 23: 1,\n",
       " 24: 1,\n",
       " 25: 1,\n",
       " 26: 1,\n",
       " 27: 1,\n",
       " 28: 1,\n",
       " 29: 1,\n",
       " 30: 1,\n",
       " 31: 1,\n",
       " 33: 1,\n",
       " 42: 1,\n",
       " 44: 1,\n",
       " 32: 2,\n",
       " 34: 2,\n",
       " 3: 2,\n",
       " 36: 2,\n",
       " 9: 2,\n",
       " 10: 2,\n",
       " 11: 2,\n",
       " 43: 2,\n",
       " 13: 2,\n",
       " 14: 2,\n",
       " 12: 2,\n",
       " 17: 2,\n",
       " 18: 2,\n",
       " 19: 2,\n",
       " 20: 2}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "part_evt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2db4eb04-cfb9-48c0-a85f-baac98b9b6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PRE communities (total 3) ---\n",
      "community 00 (10 nodes): ['ondrej-buddeus_chatroom-the-history-of-the-coca-cola-trademark-in-central-and-eastern-europe-from-1990-to-2011.txt', 'oriette_dangelo_caracas_in_your_absence_i_know_ill_find_my_name_from_spanish_trans_lupita_eydetucker_poem_1.txt', 'oriette_dangelo_criminal_syndrome_from_spanish_trans_lupita_eydetucker_poem_5.txt', 'place_lupe-gomez_trans-erin-moure_secret-energy_hermida-galicia-&-germany.txt', \"place_martin-glaz-serup_roman-nights_rome-venice-colle-di-val-d'elsa-italy-&-nicaragua-&-russia-&-usa-&-amager-denmark-&-sarajevo-bosnia-&-costa-brava-spain-&-gothenburg-sweden-&-berlin-germany.txt\", 'publishing_sphere_reader_erin_moure_reading_translation_multilingual_poetry.txt', 'publishing_sphere_reader_lionel_ruffel_contemporary_moment.txt', 'publishing_sphere_reader_nick_thurston_subcontract.txt', 'satu-taskinen_bio.txt', 'satu-taskinen_interview.txt']\n",
      "community 01 (14 nodes): ['osip_mandelstam_january_9_1937_from_russian_trans_john_high_and_matvei_yankelevich_poem_2.txt', 'osip_mandelstam_meganom_from_russian_trans_alistair_noon_poem_1.txt', 'osip_mandelstam_rome_from_russian_trans_john_high_and_matvei_yankelevich_poem_1.txt', 'paulo_leminski_reportedly_from_portuguese_trans_elisa_wouk_almino_poem_4.txt', 'place_margento_tower_mask_cannes_france.txt', 'place_oliver_goldsmith_france.txt', 'place_rachel_blau_duplessis_draft_65_that_japanese_language_tao.txt', 'place_rachel_by_matthew_arnold_paris_provence_french_riviera.txt', 'qiu_jin_pusaman_a_message_for_a_female_friend_from_chinese_trans_yilin_wang_poem_1.txt', 'qiu_jin_reflections_from_chinese_trans_yilin_wang_poem_3.txt', 'qiu_jin_to_drink_from_chinese_trans_yilin_wang_poem_5.txt', 'rosa_chávez_in_our_palm_lines_our_tenderness_is_written_from_spanish_trans_gabriela_ramirezchavez_poem_6.txt', 'rosa_chávez_may_my_heart_bloom_when_it_stops_pumping_red_ink_may_it_burst_into_small_thorns_from_spanish_trans_gabriela_ramirezchavez_poem_9.txt', 'sagawa_chika_afternoon_from_japanese_trans_sawako_nakayasu_poem_1.txt']  <-- SHARED POEM HERE\n",
      "community 02 (18 nodes): ['paalhelge_haugen_61_from_norwegian_trans_julia_johanne_tolo_poem_5.txt', 'paalhelge_haugen_65_from_norwegian_trans_julia_johanne_tolo_poem_6.txt', 'paalhelge_haugen_67_from_norwegian_trans_julia_johanne_tolo_poem_7.txt', 'patron_henekou_letter_xv_to_the_spokesperson_of_parliament_from_french_trans_patron_henekou_poem_2.txt', 'paulo_leminski_sets_versus_setbacks_from_portuguese_trans_elisa_wouk_almino_poem_2.txt', 'piyush_daiya_9_from_hindi_trans_rahul_soni_poem_4.txt', 'place_abraham-sutzkever_trans-seymour-mayne_poem-about-a-herring_thrid-reich-&-warsaw-&-poland.txt', 'place_alexis-almeida_something-tells-you-to-look.txt', 'rosa_chávez_i_braid_my_hair_so_it_rests_from_spanish_trans_gabriela_ramirezchavez_poem_4.txt', 'rose_ausländer_unity_of_dust_from_german_trans_carlie_hoffman_poem_3.txt', 'rose_ausländer_without_a_visa_from_german_trans_carlie_hoffman_poem_2.txt', 'roselyne_sibille_barbwire_in_your_mouth_from_french_trans_karthika_naïr_poem_4.txt', 'roselyne_sibille_i_write_to_you_from_the_other_side_of_the_shadow_from_french_trans_karthika_naïr_poem_1.txt', 'roselyne_sibille_latitudes_flee_from_french_trans_karthika_naïr_poem_2.txt', 'roselyne_sibille_the_shadow_moans_from_french_trans_karthika_naïr_poem_3.txt'] ...\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "--- EVENT communities (total 3) ---\n",
      "community 00 (10 nodes): ['oriette_dangelo_too_much_calcium_from_spanish_trans_lupita_eydetucker_poem_3.txt', 'publishing_sphere_danny_snelson_heath_tracing_actor_as_network.txt', 'publishing_sphere_designabilities_blog_post.txt', 'quyên_nguyễnhoàng_a_poetics_of_sleep_2_the_no_center_of_writing_from_vietnamese_trans_quyên_nguyễnhoàng_poem_2.txt', 'salaiman_juhni_roots_from_arabic_trans_ali_kadhim_and_chris_george_poem_3.txt', 'satu-taskinen_bio.txt', 'satu-taskinen_interview.txt', 'seo_jung_hak_paper_box_from_korean_trans_megan_sungyoon_poem_4.txt', 'seo_jung_hak_surreal_machinery_and_green_trees_from_korean_trans_megan_sungyoon_poem_3.txt', 'seo_jung_hak_the_cheapest_france_in_town_from_korean_trans_megan_sungyoon_poem_6.txt']\n",
      "community 01 (20 nodes): ['oriette_dangelo_too_much_bone_from_spanish_trans_lupita_eydetucker_poem_2.txt', 'osdany_morales_what_is_your_favorite_movienbsp_from_spanish_trans_harry_bauld_poem_1.txt', 'pablo_ingberg_smoke_signals_from_spanish_trans_sarah_moses_poem_2.txt', 'pablo_ingberg_the_scene_of_the_crime_from_spanish_trans_sarah_moses_poem_3.txt', 'pamela_proietti_pinhole_from_italian_trans_donna_mancusiungaro_hart_and_stephen_eric_berry_poem_1.txt', 'pamela_proietti_sicily_from_italian_trans_donna_mancusiungaro_hart_and_stephen_eric_berry_poem_3.txt', 'pamela_proietti_tanka_n_36_from_italian_trans_donna_mancusiungaro_hart_and_stephen_eric_berry_poem_4.txt', 'quotes_tempest_dr_aidan.txt', 'quyên_nguyễnhoàng_a_poetics_of_sleep_3_a_river_secretes_many_mouths_from_vietnamese_trans_quyên_nguyễnhoàng_poem_1.txt', 'rachida_madani_xi_from_french_trans_marilyn_hacker_poem_3.txt', 'rachida_madani_xiii_from_french_trans_marilyn_hacker_poem_4.txt', 'rasool_yoonan_untitled_1_from_persian_trans_siavash_saadlou_poem_1.txt', 'raúl_zurita_felipe_zurita_from_spanish_trans_daniel_borzutzky_poem_3.txt', 'raúl_zurita_iván_zurita_from_spanish_trans_daniel_borzutzky_poem_2.txt', 'raúl_zurita_josefina_pessolo_from_spanish_trans_daniel_borzutzky_poem_5.txt'] ...\n",
      "community 02 (15 nodes): ['paalhelge_haugen_77_from_norwegian_trans_julia_johanne_tolo_poem_10.txt', 'phu_recalling_love_scenes_by_pleasant_river_from_thai_trans_noh_anothai_poem_2.txt', 'pierre_peuchmaurd_glimmers_from_french_trans_ec_belli_poem_1.txt', 'pierre_peuchmaurd_the_foam_of_lions_from_french_trans_ec_belli_poem_2.txt', 'place_oliver_goldsmith_france.txt', 'place_rachel_blau_duplessis_draft_65_that_japanese_language_tao.txt', 'place_rachel_by_matthew_arnold_paris_provence_french_riviera.txt', 'publishing_sphere_diana_hamilton_autobiography_of_fatness.txt', 'qiu_jin_inscriptions_on_my_tiny_portrait_in_mens_clothes_from_chinese_trans_yilin_wang_poem_2.txt', 'qiu_jin_pusaman_a_message_for_a_female_friend_from_chinese_trans_yilin_wang_poem_1.txt', 'qiu_jin_reflections_from_chinese_trans_yilin_wang_poem_3.txt', 'saksiri_meesomsueb_dogs_in_the_lead_from_thai_trans_noh_anothai_poem_3.txt', 'saksiri_meesomsueb_sleight_from_thai_trans_noh_anothai_poem_1.txt', 'sappho_31_trans_julia_dubnoff_trans_chris_childers_trans_anne_carson_walt_whitman_woman_waits_for_me.txt', 'shanzhai_darling.txt']  <-- SHARED POEM HERE\n",
      "-------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#          com_prev, com_evt are lists of sets of node indices (from louvain_communities)\n",
    "#          mapping_prev, mapping_evt map node-index -> poem name (we created these earlier)\n",
    "#          i_prev, i_evt are the index of the shared poem in each graph\n",
    "\n",
    "def print_communities(communities, mapping, highlight_index=None, name=\"\"):\n",
    "    print(f\"\\n--- {name} communities (total {len(communities)}) ---\")\n",
    "    for cid, comm in enumerate(communities):\n",
    "        # map numeric indices -> poem names (if mapping provided)\n",
    "        members = [mapping.get(n, str(n)) for n in sorted(comm)]\n",
    "        marker = \"\"\n",
    "        if highlight_index is not None and highlight_index in comm:\n",
    "            marker = \"  <-- SHARED POEM HERE\"\n",
    "        print(f\"community {cid:02d} ({len(members)} nodes): {members[:15]}{'' if len(members)<=15 else ' ...'}{marker}\")\n",
    "    print(\"-------------------------------------------------\\n\")\n",
    "\n",
    "\n",
    "mapping_prev = {i: valid_poems_prev[i] for i in range(len(valid_poems_prev))}\n",
    "mapping_evt  = {i: valid_poems[i] for i in range(len(valid_poems))}\n",
    "print_communities(com_prev, mapping_prev, highlight_index=i_prev, name=\"PRE\")\n",
    "print_communities(com_evt,  mapping_evt,  highlight_index=i_evt,  name=\"EVENT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1c60de7e-b333-468d-9b73-695241d94ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shared poem community PRE: 1 ['osip_mandelstam_january_9_1937_from_russian_trans_john_high_and_matvei_yankelevich_poem_2.txt', 'osip_mandelstam_meganom_from_russian_trans_alistair_noon_poem_1.txt', 'osip_mandelstam_rome_from_russian_trans_john_high_and_matvei_yankelevich_poem_1.txt', 'paulo_leminski_reportedly_from_portuguese_trans_elisa_wouk_almino_poem_4.txt', 'place_margento_tower_mask_cannes_france.txt', 'place_oliver_goldsmith_france.txt', 'place_rachel_blau_duplessis_draft_65_that_japanese_language_tao.txt', 'place_rachel_by_matthew_arnold_paris_provence_french_riviera.txt', 'qiu_jin_pusaman_a_message_for_a_female_friend_from_chinese_trans_yilin_wang_poem_1.txt', 'qiu_jin_reflections_from_chinese_trans_yilin_wang_poem_3.txt', 'qiu_jin_to_drink_from_chinese_trans_yilin_wang_poem_5.txt', 'rosa_chávez_in_our_palm_lines_our_tenderness_is_written_from_spanish_trans_gabriela_ramirezchavez_poem_6.txt', 'rosa_chávez_may_my_heart_bloom_when_it_stops_pumping_red_ink_may_it_burst_into_small_thorns_from_spanish_trans_gabriela_ramirezchavez_poem_9.txt', 'sagawa_chika_afternoon_from_japanese_trans_sawako_nakayasu_poem_1.txt']\n",
      "Shared poem community EVENT: 2 ['paalhelge_haugen_77_from_norwegian_trans_julia_johanne_tolo_poem_10.txt', 'phu_recalling_love_scenes_by_pleasant_river_from_thai_trans_noh_anothai_poem_2.txt', 'pierre_peuchmaurd_glimmers_from_french_trans_ec_belli_poem_1.txt', 'pierre_peuchmaurd_the_foam_of_lions_from_french_trans_ec_belli_poem_2.txt', 'place_oliver_goldsmith_france.txt', 'place_rachel_blau_duplessis_draft_65_that_japanese_language_tao.txt', 'place_rachel_by_matthew_arnold_paris_provence_french_riviera.txt', 'publishing_sphere_diana_hamilton_autobiography_of_fatness.txt', 'qiu_jin_inscriptions_on_my_tiny_portrait_in_mens_clothes_from_chinese_trans_yilin_wang_poem_2.txt', 'qiu_jin_pusaman_a_message_for_a_female_friend_from_chinese_trans_yilin_wang_poem_1.txt', 'qiu_jin_reflections_from_chinese_trans_yilin_wang_poem_3.txt', 'saksiri_meesomsueb_dogs_in_the_lead_from_thai_trans_noh_anothai_poem_3.txt', 'saksiri_meesomsueb_sleight_from_thai_trans_noh_anothai_poem_1.txt', 'sappho_31_trans_julia_dubnoff_trans_chris_childers_trans_anne_carson_walt_whitman_woman_waits_for_me.txt', 'shanzhai_darling.txt']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def get_community_of_node(communities, node_index):\n",
    "    for cid, comm in enumerate(communities):\n",
    "        if node_index in comm:\n",
    "            return cid, sorted(list(comm))\n",
    "    return None, []\n",
    "\n",
    "cid_prev, members_prev = get_community_of_node(com_prev, i_prev)\n",
    "cid_evt,  members_evt  = get_community_of_node(com_evt,  i_evt)\n",
    "print(\"Shared poem community PRE:\", cid_prev, [mapping_prev.get(n) for n in members_prev])\n",
    "print(\"Shared poem community EVENT:\", cid_evt, [mapping_evt.get(n) for n in members_evt])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c783651f-cfea-4ca6-85b4-41a27674abfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inters_communities_unique_poem = set([mapping_prev.get(n) for n in members_prev]).intersection(set([mapping_evt.get(n) for n in members_evt]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ed371125-2937-494f-b051-014b1385b027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'place_oliver_goldsmith_france.txt',\n",
       " 'place_rachel_blau_duplessis_draft_65_that_japanese_language_tao.txt',\n",
       " 'place_rachel_by_matthew_arnold_paris_provence_french_riviera.txt',\n",
       " 'qiu_jin_pusaman_a_message_for_a_female_friend_from_chinese_trans_yilin_wang_poem_1.txt',\n",
       " 'qiu_jin_reflections_from_chinese_trans_yilin_wang_poem_3.txt'}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inters_communities_unique_poem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ed03dcb3-a028-4811-8b67-28245ff3eb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvector centrality (EVENT): 0.19246052431567542\n",
      "\n",
      "--- Direct neighbors in EVENT network ---\n",
      "Neighbor idx  10: pierre_peuchmaurd_glimmers_from_french_trans_ec_belli_poem_1.txt\n",
      "   edge weight w_ij = 0.5495\n",
      "   eigenvector c_j  = 0.2955\n",
      "   influence = w_ij * c_j = 0.1623\n",
      "Neighbor idx   9: phu_recalling_love_scenes_by_pleasant_river_from_thai_trans_noh_anothai_poem_2.txt\n",
      "   edge weight w_ij = 0.4520\n",
      "   eigenvector c_j  = 0.3095\n",
      "   influence = w_ij * c_j = 0.1399\n",
      "Neighbor idx  11: pierre_peuchmaurd_the_foam_of_lions_from_french_trans_ec_belli_poem_2.txt\n",
      "   edge weight w_ij = 0.4345\n",
      "   eigenvector c_j  = 0.2955\n",
      "   influence = w_ij * c_j = 0.1284\n",
      "Neighbor idx  14: place_rachel_by_matthew_arnold_paris_provence_french_riviera.txt\n",
      "   edge weight w_ij = 0.3915\n",
      "   eigenvector c_j  = 0.2476\n",
      "   influence = w_ij * c_j = 0.0970\n",
      "Neighbor idx  20: qiu_jin_reflections_from_chinese_trans_yilin_wang_poem_3.txt\n",
      "   edge weight w_ij = 0.4629\n",
      "   eigenvector c_j  = 0.1939\n",
      "   influence = w_ij * c_j = 0.0897\n",
      "Neighbor idx  32: saksiri_meesomsueb_dogs_in_the_lead_from_thai_trans_noh_anothai_poem_3.txt\n",
      "   edge weight w_ij = 0.4780\n",
      "   eigenvector c_j  = 0.2698\n",
      "   influence = w_ij * c_j = 0.1290\n",
      "Neighbor idx  34: saksiri_meesomsueb_sleight_from_thai_trans_noh_anothai_poem_1.txt\n",
      "   edge weight w_ij = 0.4281\n",
      "   eigenvector c_j  = 0.2597\n",
      "   influence = w_ij * c_j = 0.1112\n",
      "Neighbor idx  36: sappho_31_trans_julia_dubnoff_trans_chris_childers_trans_anne_carson_walt_whitman_woman_waits_for_me.txt\n",
      "   edge weight w_ij = 0.4227\n",
      "   eigenvector c_j  = 0.2598\n",
      "   influence = w_ij * c_j = 0.1098\n",
      "Neighbor idx  17: publishing_sphere_diana_hamilton_autobiography_of_fatness.txt\n",
      "   edge weight w_ij = 0.3572\n",
      "   eigenvector c_j  = 0.1616\n",
      "   influence = w_ij * c_j = 0.0577\n",
      "Neighbor idx  43: shanzhai_darling.txt\n",
      "   edge weight w_ij = 0.3468\n",
      "   eigenvector c_j  = 0.1250\n",
      "   influence = w_ij * c_j = 0.0434\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------------------------------------\n",
    "# IDENTIFY \"IMPORTANT\" NEIGHBORS OF THE UNIQUES SHARED POEM (that made it from prev window to the event/singularity one)\n",
    "# in the EVENT network\n",
    "# -----------------------------------------------------\n",
    "\n",
    "\n",
    "# --- 2) Compute eigenvector centrality ---\n",
    "eig_evt = nx.eigenvector_centrality_numpy(G_evt, weight='weight')\n",
    "# eig_evt  = nx.eigenvector_centrality_numpy(G_evt,  weight='weight')[i_evt]\n",
    "eig_evt_vec = np.array([eig_evt[n] for n in range(len(valid_poems))])\n",
    "\n",
    "# --- 3) Get neighbors of the shared poem ---\n",
    "neighbors = list(G_evt.neighbors(i_evt))\n",
    "\n",
    "mapping_evt  = {i: valid_poems[i] for i in range(len(valid_poems))}\n",
    "print(\"Eigenvector centrality (EVENT):\", eig_evt[i_evt])\n",
    "print(\"\\n--- Direct neighbors in EVENT network ---\")\n",
    "\n",
    "records = []\n",
    "\n",
    "for j in neighbors:\n",
    "    w = G_evt[i_evt][j]['weight']\n",
    "    c = eig_evt[j]\n",
    "    influence = w * c\n",
    "    records.append((j, valid_poems[j], w, c, influence))\n",
    "    print(f\"Neighbor idx {j:3d}: {valid_poems[j]}\")\n",
    "    print(f\"   edge weight w_ij = {w:.4f}\")\n",
    "    print(f\"   eigenvector c_j  = {c:.4f}\")\n",
    "    print(f\"   influence = w_ij * c_j = {influence:.4f}\")\n",
    "\n",
    "# --- 4) Sort by influence (descending) ---\n",
    "records_sorted = sorted(records, key=lambda x: x[4], reverse=True)\n",
    "\n",
    "# Optional: return as a DataFrame\n",
    "# try:\n",
    "    # import pandas as pd\n",
    "    # df_inf = pd.DataFrame(records_sorted, columns=[\"index\",\"poem\",\"weight\",\"eig_centrality\",\"influence\"])\n",
    "    # display(df_inf.head(20))\n",
    "# except ImportError:\n",
    "    # pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "306941f0-b03a-405c-a2f3-90df9527a656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10,\n",
       "  'pierre_peuchmaurd_glimmers_from_french_trans_ec_belli_poem_1.txt',\n",
       "  0.5494834160567819,\n",
       "  0.29545431439707576,\n",
       "  0.1623472459636196),\n",
       " (9,\n",
       "  'phu_recalling_love_scenes_by_pleasant_river_from_thai_trans_noh_anothai_poem_2.txt',\n",
       "  0.45195930441337306,\n",
       "  0.30952723187264153,\n",
       "  0.1398937124141559),\n",
       " (32,\n",
       "  'saksiri_meesomsueb_dogs_in_the_lead_from_thai_trans_noh_anothai_poem_3.txt',\n",
       "  0.4780074949085418,\n",
       "  0.2697973995102332,\n",
       "  0.1289651790727256),\n",
       " (11,\n",
       "  'pierre_peuchmaurd_the_foam_of_lions_from_french_trans_ec_belli_poem_2.txt',\n",
       "  0.4344534651858002,\n",
       "  0.29546166370931853,\n",
       "  0.12836434362807503),\n",
       " (34,\n",
       "  'saksiri_meesomsueb_sleight_from_thai_trans_noh_anothai_poem_1.txt',\n",
       "  0.4280549793693069,\n",
       "  0.25973901153304946,\n",
       "  0.11118257722318366),\n",
       " (36,\n",
       "  'sappho_31_trans_julia_dubnoff_trans_chris_childers_trans_anne_carson_walt_whitman_woman_waits_for_me.txt',\n",
       "  0.42265278784838384,\n",
       "  0.2597628094732154,\n",
       "  0.10978947560318306),\n",
       " (14,\n",
       "  'place_rachel_by_matthew_arnold_paris_provence_french_riviera.txt',\n",
       "  0.3915472452427897,\n",
       "  0.24762431472431712,\n",
       "  0.09695661828543994),\n",
       " (20,\n",
       "  'qiu_jin_reflections_from_chinese_trans_yilin_wang_poem_3.txt',\n",
       "  0.4628862133077504,\n",
       "  0.19385863585077112,\n",
       "  0.08973448986596956),\n",
       " (17,\n",
       "  'publishing_sphere_diana_hamilton_autobiography_of_fatness.txt',\n",
       "  0.3572260176041789,\n",
       "  0.16161911391607608,\n",
       "  0.057734552432955985),\n",
       " (43,\n",
       "  'shanzhai_darling.txt',\n",
       "  0.34678712739093703,\n",
       "  0.12504233751497343,\n",
       "  0.043363073029065634)]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "records_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35b50c7-b1ca-4924-b0cf-09f89fa69f13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "dd7b0a56-d18e-4204-92f3-c29ad47dfcb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PRE-EVENT neighbors for shared poem place_rachel_blau_duplessis_draft_65_that_japanese_language_tao.txt ===\n",
      "Eigenvector centrality (PRE): 0.0326168804870135\n",
      "\n",
      "--- Direct neighbors in PRE-EVENT network ---\n",
      "Neighbor idx   5: osip_mandelstam_rome_from_russian_trans_john_high_and_matvei_yankelevich_poem_1.txt\n",
      "   edge weight w_ij = 0.4607\n",
      "   eigenvector c_j  = 0.0372\n",
      "   influence = w_ij * c_j = 0.0171\n",
      "Neighbor idx  15: place_lupe-gomez_trans-erin-moure_secret-energy_hermida-galicia-&-germany.txt\n",
      "   edge weight w_ij = 0.4034\n",
      "   eigenvector c_j  = 0.0884\n",
      "   influence = w_ij * c_j = 0.0356\n",
      "Neighbor idx  16: place_margento_tower_mask_cannes_france.txt\n",
      "   edge weight w_ij = 0.2799\n",
      "   eigenvector c_j  = 0.0316\n",
      "   influence = w_ij * c_j = 0.0089\n",
      "Neighbor idx  17: place_martin-glaz-serup_roman-nights_rome-venice-colle-di-val-d'elsa-italy-&-nicaragua-&-russia-&-usa-&-amager-denmark-&-sarajevo-bosnia-&-costa-brava-spain-&-gothenburg-sweden-&-berlin-germany.txt\n",
      "   edge weight w_ij = 0.2611\n",
      "   eigenvector c_j  = 0.0100\n",
      "   influence = w_ij * c_j = 0.0026\n",
      "Neighbor idx   3: osip_mandelstam_january_9_1937_from_russian_trans_john_high_and_matvei_yankelevich_poem_2.txt\n",
      "   edge weight w_ij = 0.2696\n",
      "   eigenvector c_j  = 0.0807\n",
      "   influence = w_ij * c_j = 0.0218\n",
      "Neighbor idx   4: osip_mandelstam_meganom_from_russian_trans_alistair_noon_poem_1.txt\n",
      "   edge weight w_ij = 0.3311\n",
      "   eigenvector c_j  = 0.1126\n",
      "   influence = w_ij * c_j = 0.0373\n",
      "Neighbor idx  10: paulo_leminski_reportedly_from_portuguese_trans_elisa_wouk_almino_poem_4.txt\n",
      "   edge weight w_ij = 0.3189\n",
      "   eigenvector c_j  = 0.0466\n",
      "   influence = w_ij * c_j = 0.0148\n",
      "Neighbor idx  18: place_oliver_goldsmith_france.txt\n",
      "   edge weight w_ij = 0.3098\n",
      "   eigenvector c_j  = 0.0584\n",
      "   influence = w_ij * c_j = 0.0181\n",
      "Neighbor idx  20: place_rachel_by_matthew_arnold_paris_provence_french_riviera.txt\n",
      "   edge weight w_ij = 0.3876\n",
      "   eigenvector c_j  = 0.0509\n",
      "   influence = w_ij * c_j = 0.0197\n",
      "Neighbor idx  21: publishing_sphere_reader_erin_moure_reading_translation_multilingual_poetry.txt\n",
      "   edge weight w_ij = 0.1438\n",
      "   eigenvector c_j  = 0.0123\n",
      "   influence = w_ij * c_j = 0.0018\n",
      "Neighbor idx  23: publishing_sphere_reader_nick_thurston_subcontract.txt\n",
      "   edge weight w_ij = 0.0482\n",
      "   eigenvector c_j  = 0.0040\n",
      "   influence = w_ij * c_j = 0.0002\n",
      "\n",
      "=== MOST INFLUENTIAL PRE-EVENT NEIGHBORS (ranked) ===\n",
      "osip_mandelstam_meganom_from_russian_trans_alistair_noon_poem_1.txt | w=0.3311 | c=0.1126 | influence=0.0373\n",
      "place_lupe-gomez_trans-erin-moure_secret-energy_hermida-galicia-&-germany.txt | w=0.4034 | c=0.0884 | influence=0.0356\n",
      "osip_mandelstam_january_9_1937_from_russian_trans_john_high_and_matvei_yankelevich_poem_2.txt | w=0.2696 | c=0.0807 | influence=0.0218\n",
      "place_rachel_by_matthew_arnold_paris_provence_french_riviera.txt | w=0.3876 | c=0.0509 | influence=0.0197\n",
      "place_oliver_goldsmith_france.txt                            | w=0.3098 | c=0.0584 | influence=0.0181\n",
      "osip_mandelstam_rome_from_russian_trans_john_high_and_matvei_yankelevich_poem_1.txt | w=0.4607 | c=0.0372 | influence=0.0171\n",
      "paulo_leminski_reportedly_from_portuguese_trans_elisa_wouk_almino_poem_4.txt | w=0.3189 | c=0.0466 | influence=0.0148\n",
      "place_margento_tower_mask_cannes_france.txt                  | w=0.2799 | c=0.0316 | influence=0.0089\n",
      "place_martin-glaz-serup_roman-nights_rome-venice-colle-di-val-d'elsa-italy-&-nicaragua-&-russia-&-usa-&-amager-denmark-&-sarajevo-bosnia-&-costa-brava-spain-&-gothenburg-sweden-&-berlin-germany.txt | w=0.2611 | c=0.0100 | influence=0.0026\n",
      "publishing_sphere_reader_erin_moure_reading_translation_multilingual_poetry.txt | w=0.1438 | c=0.0123 | influence=0.0018\n",
      "publishing_sphere_reader_nick_thurston_subcontract.txt       | w=0.0482 | c=0.0040 | influence=0.0002\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------------------------------------\n",
    "# PRE-EVENT NEIGHBOR ANALYSIS (the ties the poem had before singularity)\n",
    "# -----------------------------------------------------\n",
    "\n",
    "\n",
    "# Eigenvector centrality (pre-event)\n",
    "eig_prev = nx.eigenvector_centrality_numpy(G_prev, weight='weight')\n",
    "eig_prev_vec = np.array([eig_prev[n] for n in range(len(valid_poems_prev))])\n",
    "\n",
    "# Neighbors of shared poem in the PRE network\n",
    "neighbors_prev = list(G_prev.neighbors(i_prev))\n",
    "\n",
    "print(f\"\\n=== PRE-EVENT neighbors for shared poem {valid_poems_prev[i_prev]} ===\")\n",
    "print(\"Eigenvector centrality (PRE):\", eig_prev[i_prev])\n",
    "print(\"\\n--- Direct neighbors in PRE-EVENT network ---\")\n",
    "\n",
    "records_prev = []\n",
    "\n",
    "for j in neighbors_prev:\n",
    "    w = G_prev[i_prev][j]['weight']\n",
    "    c = eig_prev_vec[j]\n",
    "    influence = w * c\n",
    "    records_prev.append((j, valid_poems_prev[j], w, c, influence))\n",
    "    \n",
    "    print(f\"Neighbor idx {j:3d}: {valid_poems_prev[j]}\")\n",
    "    print(f\"   edge weight w_ij = {w:.4f}\")\n",
    "    print(f\"   eigenvector c_j  = {c:.4f}\")\n",
    "    print(f\"   influence = w_ij * c_j = {influence:.4f}\")\n",
    "\n",
    "# Sort by influence\n",
    "records_prev_sorted = sorted(records_prev, key=lambda x: x[4], reverse=True)\n",
    "\n",
    "print(\"\\n=== MOST INFLUENTIAL PRE-EVENT NEIGHBORS (ranked) ===\")\n",
    "for (j, name, w, c, infl) in records_prev_sorted:\n",
    "    print(f\"{name:60s} | w={w:.4f} | c={c:.4f} | influence={infl:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e1854c-dc14-487f-983d-2158aa68a1c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dea055-f1a2-4e67-afb9-19eb5ffa0507",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "605b6cfb-bdd1-400c-8fdc-54c82cb46733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== K-CORE INDEX ===\n",
      "Core PRE: 8\n",
      "Core EVENT: 8\n",
      "Δ core (event - pre): 0\n",
      "\n",
      "=== LOCAL ASSORTATIVITY ===\n",
      "Local assortativity PRE: -0.1432894420850021\n",
      "Local assortativity EVENT: -0.2136156012179899\n",
      "Δ assortativity: -0.07032615913298779\n",
      "\n",
      "=== SPECTRAL EMBEDDING SHIFT ===\n",
      "Coordinates PRE: [-0.13688314  0.0632467   0.02723614]\n",
      "Coordinates EVENT: [-0.12686304  0.14096015  0.01220931]\n",
      "L2 shift: 0.07978463356769269\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------- K-CORE INDEX ----------\n",
    "core_prev = nx.core_number(G_prev).get(i_prev, 0)\n",
    "core_evt  = nx.core_number(G_evt).get(i_evt, 0)\n",
    "\n",
    "print(\"\\n=== K-CORE INDEX ===\")\n",
    "print(\"Core PRE:\", core_prev)\n",
    "print(\"Core EVENT:\", core_evt)\n",
    "print(\"Δ core (event - pre):\", core_evt - core_prev)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# ---------- LOCAL ASSORTATIVITY ----------\n",
    "def local_assortativity(G, node):\n",
    "    \"\"\"Compute degree assortativity restricted to the 1-hop ego network of `node`.\"\"\"\n",
    "    neighbors = list(G.neighbors(node))\n",
    "    if len(neighbors) < 2:\n",
    "        return 0.0  # undefined for nodes with <2 neighbors\n",
    "\n",
    "    # Extract subgraph of node + neighbors\n",
    "    ego = G.subgraph([node] + neighbors)\n",
    "    \n",
    "    # Node degrees in this subgraph\n",
    "    deg = dict(ego.degree())\n",
    "\n",
    "    # Collect degree pairs for all edges\n",
    "    deg_pairs = []\n",
    "    for u, v in ego.edges():\n",
    "        deg_pairs.append((deg[u], deg[v]))\n",
    "\n",
    "    if len(deg_pairs) < 2:\n",
    "        return 0.0\n",
    "\n",
    "    xs = np.array([d1 for d1, d2 in deg_pairs], dtype=float)\n",
    "    ys = np.array([d2 for d1, d2 in deg_pairs], dtype=float)\n",
    "\n",
    "    if xs.std() == 0 or ys.std() == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return np.corrcoef(xs, ys)[0, 1]\n",
    "\n",
    "\n",
    "loc_assort_prev = local_assortativity(G_prev, i_prev)\n",
    "loc_assort_evt  = local_assortativity(G_evt,  i_evt)\n",
    "\n",
    "print(\"\\n=== LOCAL ASSORTATIVITY ===\")\n",
    "print(\"Local assortativity PRE:\", loc_assort_prev)\n",
    "print(\"Local assortativity EVENT:\", loc_assort_evt)\n",
    "print(\"Δ assortativity:\", loc_assort_evt - loc_assort_prev)\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from scipy.sparse import csgraph\n",
    "\n",
    "# ---------- SPECTRAL EMBEDDING (3D) ----------\n",
    "\n",
    "def spectral_embedding_3d(G):\n",
    "    # Weighted adjacency matrix\n",
    "    A = nx.to_scipy_sparse_array(G, weight='weight', dtype=float)\n",
    "\n",
    "    # Normalized Laplacian\n",
    "    L = csgraph.laplacian(A, normed=True)\n",
    "\n",
    "    # Eigenvectors\n",
    "    eigenvals, eigenvecs = np.linalg.eigh(L.toarray())\n",
    "\n",
    "    # Skip the first eigenvector (zero eigenvalue)\n",
    "    vecs = eigenvecs[:, 1:4]\n",
    "    return vecs\n",
    "\n",
    "\n",
    "# Compute embeddings\n",
    "spec_prev = spectral_embedding_3d(G_prev)\n",
    "spec_evt  = spectral_embedding_3d(G_evt)\n",
    "\n",
    "# Coordinates of the poem\n",
    "coord_prev = spec_prev[i_prev]\n",
    "coord_evt  = spec_evt[i_evt]\n",
    "\n",
    "# Euclidean shift\n",
    "spec_shift = np.linalg.norm(coord_prev - coord_evt)\n",
    "\n",
    "print(\"\\n=== SPECTRAL EMBEDDING SHIFT ===\")\n",
    "print(\"Coordinates PRE:\", coord_prev)\n",
    "print(\"Coordinates EVENT:\", coord_evt)\n",
    "print(\"L2 shift:\", spec_shift)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f5e5bf-dac5-478b-a556-6fefc2c67b5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0f5bfd73-9090-4ac7-b4ff-0840cc3fc0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean semantic cosine distance PRE: 0.7544712871313095\n",
      "Mean semantic cosine distance EVENT: 0.8123135417699814\n",
      "Density PRE: 0.1951219512195122\n",
      "Density EVENT: 0.18181818181818182\n",
      "Avg edge weight PRE: 0.44997312298129255\n",
      "Avg edge weight EVENT: 0.4567532805084696\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# POEM-SPACE GLOBAL GEOMETRY CHANGES\n",
    "\n",
    "# average pairwise semantic distance\n",
    "mean_sem_dist_prev = float(1 - np.mean(cosine_similarity(emb_prev)))\n",
    "mean_sem_dist_evt  = float(1 - np.mean(cosine_similarity(emb)))\n",
    "\n",
    "print(\"Mean semantic cosine distance PRE:\", mean_sem_dist_prev)\n",
    "print(\"Mean semantic cosine distance EVENT:\", mean_sem_dist_evt)\n",
    "\n",
    "# density\n",
    "def graph_density(A):\n",
    "    P = A.shape[0]\n",
    "    possible = P*(P-1)\n",
    "    actual = A.count_nonzero()\n",
    "    return actual / possible\n",
    "\n",
    "# edge weights\n",
    "def avg_edge_weight(A):\n",
    "    if A.count_nonzero() == 0:\n",
    "        return 0\n",
    "    return float(A.data.mean())\n",
    "\n",
    "print(\"Density PRE:\", graph_density(A_sem_subset_prev))\n",
    "print(\"Density EVENT:\", graph_density(A_sem_subset))\n",
    "print(\"Avg edge weight PRE:\", avg_edge_weight(A_sem_subset_prev))\n",
    "print(\"Avg edge weight EVENT:\", avg_edge_weight(A_sem_subset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee487c7e-60ec-456c-9ca9-68f034a6fbb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6b69b32d-4bf1-424a-9d94-3c652ffa5e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.save(\"margento_graphpoem_dhsi23_embeddings_window_prev_to_singularity_3.npy\", emb_prev)\n",
    "np.save(\"margento_graphpoem_dhsi23_embeddings_singularity_3.npy\", emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960dfec3-5ad1-48dc-83f9-0426e2279b6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7029c87-3fa9-4909-b4cc-e3d74ae944e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f175d79f-9caa-458f-8d4f-2c97186c0916",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ANALYZING THE USERS NOW AS THEY TRANSITION FROM PRE-WINDOW TO EVENT\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# ---- Helper: build users↔poems bipartite adjacency for a window ----\n",
    "def build_user_poem(df_tw, df_jh, df_fb, poems_list, window):\n",
    "    s, e = window\n",
    "\n",
    "    # slice logs\n",
    "    tw_slice = df_tw[(df_tw[\"time\"] >= s) & (df_tw[\"time\"] <= e)]\n",
    "    jh_slice = df_jh[(df_jh[\"time\"] >= s) & (df_jh[\"time\"] <= e)]\n",
    "    fb_slice = df_fb[(df_fb[\"time\"] >= s) & (df_fb[\"time\"] <= e)]\n",
    "\n",
    "    # unify poem column for each df\n",
    "    # TW + JH have \"poem\" or \"file\"\n",
    "    def get_poem_column(df):\n",
    "        if \"poem\" in df.columns:\n",
    "            return \"poem\"\n",
    "        if \"file\" in df.columns:\n",
    "            return \"file\"\n",
    "        return None\n",
    "\n",
    "    col_tw = get_poem_column(tw_slice)\n",
    "    col_jh = get_poem_column(jh_slice)\n",
    "    col_fb = get_poem_column(fb_slice)\n",
    "\n",
    "    # For FB: map to poems by nearest Twitter bot tweet (if no poem column)\n",
    "    if col_fb is None:\n",
    "        tweet_times = df_tw[[\"time\",\"poem\"]].sort_values(\"time\")\n",
    "        def map_time_to_poem(t):\n",
    "            prev = tweet_times[tweet_times[\"time\"] <= t]\n",
    "            if len(prev) > 0:\n",
    "                return prev.iloc[-1][\"poem\"]\n",
    "            else:\n",
    "                return None\n",
    "        fb_slice = fb_slice.copy()\n",
    "        fb_slice[\"poem\"] = fb_slice[\"time\"].apply(map_time_to_poem)\n",
    "        col_fb = \"poem\"\n",
    "\n",
    "    # Filter only the poems in our subset\n",
    "    tw_slice = tw_slice[tw_slice[col_tw].isin(poems_list)]\n",
    "    jh_slice = jh_slice[jh_slice[col_jh].isin(poems_list)]\n",
    "    fb_slice = fb_slice[fb_slice[col_fb].isin(poems_list)]\n",
    "\n",
    "    # collect users\n",
    "    users = sorted(set(tw_slice[\"user\"]) |\n",
    "                   set(jh_slice[\"user\"]) |\n",
    "                   set(fb_slice[\"user\"]))\n",
    "\n",
    "    U = len(users)\n",
    "    P = len(poems_list)\n",
    "    uidx = {u:i for i,u in enumerate(users)}\n",
    "    pidx = {p:i for i,p in enumerate(poems_list)}\n",
    "\n",
    "    rows=[]; cols=[]; data=[]\n",
    "\n",
    "    # define platform weights\n",
    "    w_tw = 1.0\n",
    "    w_jh = 1.2      # coding platform often has stronger weight\n",
    "    w_fb = 0.7      # livestream tends to be supplementary\n",
    "\n",
    "    # TW\n",
    "    for _,r in tw_slice.iterrows():\n",
    "        rows.append(uidx[r[\"user\"]])\n",
    "        cols.append(pidx[r[col_tw]])\n",
    "        data.append(w_tw)\n",
    "\n",
    "    # JH\n",
    "    for _,r in jh_slice.iterrows():\n",
    "        rows.append(uidx[r[\"user\"]])\n",
    "        cols.append(pidx[r[col_jh]])\n",
    "        data.append(w_jh)\n",
    "\n",
    "    # FB\n",
    "    for _,r in fb_slice.iterrows():\n",
    "        rows.append(uidx[r[\"user\"]])\n",
    "        cols.append(pidx[r[col_fb]])\n",
    "        data.append(w_fb)\n",
    "\n",
    "    A_up = sp.csr_matrix((data,(rows,cols)), shape=(U,P))\n",
    "\n",
    "    return A_up, users\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ae15d19a-f925-484d-a1a2-1db9f0c066a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# USERS & POEMS IN PREV WINDOW\n",
    "A_up_prev, users_prev = build_user_poem(df_tw, df_jh, df_fb, poems, pre_short) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dea74602-f5b5-4658-9374-e36f8c7e8755",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# USERS & POEMS IN EVENT WINDOW\n",
    "A_up, users = build_user_poem(df_tw, df_jh, df_fb, poems, ev[\"event_window\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0ffc7ed9-8975-4047-ab6c-aa242eb25e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.save(\"margento_graphpoem_dhsi23_adjacency_matrix_window_prev_to_singularity_3.npy\", A_up_prev)\n",
    "np.save(\"margento_graphpoem_dhsi23_adjacency_matrix_singularity_3.npy\", A_up_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2215743c-843e-45cb-ace8-45edf728508f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fb_user_8', 'jh_user_101']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a8bc7922-87d3-47c0-a4e0-358f06d3e661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"margento_graphpoem_dhsi23_users_window_prev_to_singularity_3.pkl\", \"wb\") as f:\n",
    "    pickle.dump(users_prev, f)\n",
    "\n",
    "with open(\"margento_graphpoem_dhsi23_users_singularity_3.pkl\", \"wb\") as fo:\n",
    "    pickle.dump(users, fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4c5470-e82e-4073-a6fd-0742e4891905",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0977a96d-ad60-4e84-ae6b-d4483d514516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean user activity PRE: 1.6999999999999997\n",
      "Mean user activity EVENT: 2.7940476190476184\n",
      "Median PRE: 1.2\n",
      "Median EVENT: 2.4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "deg_user_prev = np.array(A_up_prev.sum(axis=1)).flatten()\n",
    "deg_user_evt  = np.array(A_up.sum(axis=1)).flatten()\n",
    "\n",
    "print(\"Mean user activity PRE:\", deg_user_prev.mean())\n",
    "print(\"Mean user activity EVENT:\", deg_user_evt.mean())\n",
    "print(\"Median PRE:\", np.median(deg_user_prev))\n",
    "print(\"Median EVENT:\", np.median(deg_user_evt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7b332a61-3d8e-4803-b3ba-102e3d845aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy PRE: 4.004138318292434\n",
      "Entropy EVENT: 4.245624796802582\n",
      "Δ entropy (event - pre): 0.24148647851014804\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def participation_entropy(deg):\n",
    "    tot = deg.sum()\n",
    "    if tot == 0:\n",
    "        return 0\n",
    "    p = deg / tot\n",
    "    p = p[p>0]\n",
    "    return float(-(p*np.log(p)).sum())\n",
    "\n",
    "print(\"Entropy PRE:\", participation_entropy(deg_user_prev))\n",
    "print(\"Entropy EVENT:\", participation_entropy(deg_user_evt))\n",
    "print(\"Δ entropy (event - pre):\", participation_entropy(deg_user_evt) - participation_entropy(deg_user_prev))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f581324-3f5e-49b2-99af-27ad0507f9fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6d5db215-26d5-4322-9868-1ef2f88e1820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-20 edge overlap between windows: 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# STRUCTURAL SIGNATURE OF THE WINDOWS\n",
    "\n",
    "# top-20 strongest edges\n",
    "def top_edges(A, k=20):\n",
    "    coo = A.tocoo()\n",
    "    edges = [(i,j,w) for i,j,w in zip(coo.row, coo.col, coo.data) if i<j]\n",
    "    edges_sorted = sorted(edges, key=lambda x: -abs(x[2]))[:k]\n",
    "    return set((i,j) for i,j,_ in edges_sorted)\n",
    "\n",
    "overlap20 = len(top_edges(A_sem_subset_prev,20) & top_edges(A_sem_subset,20)) / 20\n",
    "\n",
    "print(\"Top-20 edge overlap between windows:\", overlap20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "65bae3ff-217d-4fa1-b944-e25b7a90ddb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge overlap between windows: 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def edges(A, k=20):\n",
    "    coo = A.tocoo()\n",
    "    edges = [(i,j,w) for i,j,w in zip(coo.row, coo.col, coo.data) if i<j]\n",
    "    edges_sorted = sorted(edges, key=lambda x: -abs(x[2]))[:k]\n",
    "    return set((i,j) for i,j,_ in edges_sorted)\n",
    "\n",
    "overlap_e = len(edges(A_sem_subset_prev) & edges(A_sem_subset)) / min(len(edges(A_sem_subset_prev)), len(edges(A_sem_subset)))\n",
    "\n",
    "print(\"Edge overlap between windows:\", overlap_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb13f66-3dad-4a17-82f1-cc87336f2833",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "test_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
